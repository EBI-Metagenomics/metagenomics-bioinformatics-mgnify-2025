[
  {
    "objectID": "sessions/qc.html",
    "href": "sessions/qc.html",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "",
    "text": "These instructions are for the course VM. To run externally, please refer to the section at the end.\n\n\n\n\n\n\nCautionA note about copying and pasting in the VMs\n\n\n\nYou will not be able to use Ctrl+C for copying and Ctrl+P for pasting in the VM terminals. Instead, we recommend using right click and selecting copy/paste.\n\n\n\n\n\n\n\n\nNoteCheck data is loaded correctly\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/course_dir/data_dir/qc_data/. Check if it is there. If not, go to the Downloading data section\n\n\n\n\n\n\n\n\nCautionDownloading data\n\n\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/course_dir/data_dir/qc_data. If not, you should download and decompress the data as follows:\nmkdir -p /home/training/course_dir/data_dir\ncd /home/training/course_dir/data_dir\n\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/qc-practical/qc_data.tar.gz\n\ntar -xzvf qc_data.tar.gz\n# you should now have the quality subdirectory in your /home/training/QC_session. All the data you will need to run the practicals will be in this subdirectory\n\n#you can now remove the qc_data.tar.gz\nrm qc_data.tar.gz\n\n\n\n\n\n\n\n\n\nNoteKeeping your results organised\n\n\n\nThis practical (and the others) will generate quite a few output files from the different commands. It’s therefore recommended you keep your results well organised into different subdirectories, starting with the creation of a qc_results directory that will contain everything else.\nmkdir /home/training/course_dir/work_dir/Day_2/qc_results\nYou should now have in your /home/training/course_dir directory, the following subdirectories (data_dir/qc_data/ and work_dir/Day_2/qc_results/). Make sure both are there before moving onto the next steps. You might run into permission issues later if you have not created them properly.\n\n\nFor this tutorial, you’ll need to move into the working directory (/home/training/course_dir/data_dir/qc_data) and start a Docker container. Set the variables DATADIR and RESDIR as instructed.\ncd /home/training/course_dir/data_dir/qc_data\nchmod -R 777 /home/training/course_dir/data_dir/qc_data\nexport DATADIR=/home/training/course_dir/data_dir/qc_data\nchmod -R 777 /home/training/course_dir/work_dir/Day_2/qc_results\nexport RESDIR=/home/training/course_dir/work_dir/Day_2/qc_results\nxhost +\nYou will get the message “access control disabled, clients can connect from any host”\n\n\n\n\n\n\nImportant\n\n\n\nAgain, to avoid permission issues, it’s very important that the directories $DATADIR and $RESDIR variables are exported correctly before running the container. You can check this by running:\necho $DATADIR\necho $RESDIR\nThese commands should print out the paths for those variables. If it’s not printing anything, go back to the last instruction before proceeding.\n\n\nNow start the Docker container:\ndocker run --rm -it --user 1001 -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/metagenomics-qc-practical",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#prerequisites",
    "href": "sessions/qc.html#prerequisites",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "",
    "text": "These instructions are for the course VM. To run externally, please refer to the section at the end.\n\n\n\n\n\n\nCautionA note about copying and pasting in the VMs\n\n\n\nYou will not be able to use Ctrl+C for copying and Ctrl+P for pasting in the VM terminals. Instead, we recommend using right click and selecting copy/paste.\n\n\n\n\n\n\n\n\nNoteCheck data is loaded correctly\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/course_dir/data_dir/qc_data/. Check if it is there. If not, go to the Downloading data section\n\n\n\n\n\n\n\n\nCautionDownloading data\n\n\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/course_dir/data_dir/qc_data. If not, you should download and decompress the data as follows:\nmkdir -p /home/training/course_dir/data_dir\ncd /home/training/course_dir/data_dir\n\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/qc-practical/qc_data.tar.gz\n\ntar -xzvf qc_data.tar.gz\n# you should now have the quality subdirectory in your /home/training/QC_session. All the data you will need to run the practicals will be in this subdirectory\n\n#you can now remove the qc_data.tar.gz\nrm qc_data.tar.gz\n\n\n\n\n\n\n\n\n\nNoteKeeping your results organised\n\n\n\nThis practical (and the others) will generate quite a few output files from the different commands. It’s therefore recommended you keep your results well organised into different subdirectories, starting with the creation of a qc_results directory that will contain everything else.\nmkdir /home/training/course_dir/work_dir/Day_2/qc_results\nYou should now have in your /home/training/course_dir directory, the following subdirectories (data_dir/qc_data/ and work_dir/Day_2/qc_results/). Make sure both are there before moving onto the next steps. You might run into permission issues later if you have not created them properly.\n\n\nFor this tutorial, you’ll need to move into the working directory (/home/training/course_dir/data_dir/qc_data) and start a Docker container. Set the variables DATADIR and RESDIR as instructed.\ncd /home/training/course_dir/data_dir/qc_data\nchmod -R 777 /home/training/course_dir/data_dir/qc_data\nexport DATADIR=/home/training/course_dir/data_dir/qc_data\nchmod -R 777 /home/training/course_dir/work_dir/Day_2/qc_results\nexport RESDIR=/home/training/course_dir/work_dir/Day_2/qc_results\nxhost +\nYou will get the message “access control disabled, clients can connect from any host”\n\n\n\n\n\n\nImportant\n\n\n\nAgain, to avoid permission issues, it’s very important that the directories $DATADIR and $RESDIR variables are exported correctly before running the container. You can check this by running:\necho $DATADIR\necho $RESDIR\nThese commands should print out the paths for those variables. If it’s not printing anything, go back to the last instruction before proceeding.\n\n\nNow start the Docker container:\ndocker run --rm -it --user 1001 -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/metagenomics-qc-practical",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#quality-control-and-filtering-of-the-raw-sequencing-read-files",
    "href": "sessions/qc.html#quality-control-and-filtering-of-the-raw-sequencing-read-files",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Quality control and filtering of the raw sequencing read files",
    "text": "Quality control and filtering of the raw sequencing read files\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\nIn the following exercises, you’ll learn how to check the quality of short read sequences, identify adaptor sequences, remove adapters and low-quality sequences, and construct a reference database for host decontamination.\n\n\n\n\n\n\n\n\nNoteHere you should see the contents of the working directory.\n\n\n\nThese are the files we’ll use for the practical:\nls /opt/data\n\n\n\nQuality assessment with FastQC and multiqc\nWe will start by using a tool called FastQC, which will generate a report describing multiple quality measures for the given reads.\n\n\n\n\n\n\nStepGenerate a directory of the FastQC results\n\n\n\nmkdir -p /opt/results/fastqc_results/oral\nfastqc /opt/data/oral_human_example_1_splitaa.fastq.gz -o /opt/results/fastqc_results/oral\nfastqc /opt/data/oral_human_example_2_splitaa.fastq.gz -o /opt/results/fastqc_results/oral\nchown 1001 /opt/results/fastqc_results/oral/*.html\n\n\nThe -o option used with FastQC sends the output files to the given path.\n\n\n\n\n\n\nStepNow on your computer, select the folder icon.\n\n\n\nNavigate to Files → Home → course_dir → work_dir → Day_2 → qc_results → fastqc_results → oral in your VM\nRight-click on file oral_human_example_1_splitaa_fastqc.html, select ‘open with other application’, and open with Firefox.\n\n\n\nScreenshot of FastQC\n\n\nSpend some time looking at the ‘Per base sequence quality.’\n\n\nFor each position, a BoxWhisker-type plot is drawn:\n\nThe central red line is the median value.\nThe yellow box represents the inter-quartile range (25-75%).\nThe upper and lower whiskers represent the 10% and 90% points.\nThe blue line represents the mean quality.\n\nThe y-axis on the graph shows the quality scores. The higher the score, the better the base call. The background of the graph divides the y-axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red). The quality of calls on most platforms will degrade as the run progresses, so it’s common to see base calls falling into the orange area towards the end of a read.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does this tell you about your sequence data? When do the errors start?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nFrom the distribution of quality scores, we can tell that the sequencing for these reads is a standard good quality run, with the errors mainly starting at the 3’ end of the reads being a common feature.\n\n\n\nIn the pre-processed files, we see two warnings, as shown on the left side of the report. Navigate to the “Per bases sequence content.”\n\n\n\nScreenshot of FastQC\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAt around 15-19 nucleotides, the DNA composition becomes very even; however, at the 5’ end of the sequence, there are distinct differences. Why do you think that is?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThis bias at the beginning is a feature of the library preparation method used. In this case, a transposase-based fragmentation was used to break up the DNA for sequencing, which has biased towards certain bases.\n\n\n\n\n\n\n\n\n\nStep\n\n\n\nOpen up the FastQC report corresponding to the reversed reads.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any significant differences between the forward and reverse files?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThere are no noticeable differences between the two directions, which is a good sign about the quality of the sequencing.\n\n\n\nFor more information on the FastQC report, please consult the ‘Documentation’ available from this site: FastQC Documentation\nWe are currently only looking at two files, but often we want to look at many files. The tool multiqc aggregates the FastQC results across many samples and creates a single report for easy comparison. Here we will demonstrate the use of this tool.\n\n\n\n\n\n\nStepRun\n\n\n\nmkdir -p /opt/results/multiqc_results/oral\nmultiqc /opt/results/fastqc_results/oral -o /opt/results/multiqc_results/oral\nchown 1001 /opt/results/multiqc_results/oral/*.html\n\n\nIn this case, we provide the folder containing the FastQC results to multiqc, and similar to FastQC, the -o argument allows us to set the output directory for this summarized report.\n\n\n\n\n\n\nStepNow on your computer, select the folder icon.\n\n\n\nNavigate to Home → course_dir → work_dir → Day_2 → qc_results → oral in your VM\nRight-click on file multiqc_report.html, select ‘open with other application’, and open with Firefox.\n\n\n\nScreenshot of multiQC\n\n\nScroll down through the report. The sequence quality histograms show the above results from each file as two separate lines. The ‘Status Checks’ show a matrix of which samples passed check and which ones have problems.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat fraction of reads are duplicates?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nFor both directions, the fraction of duplicate reads is 0.9%.\n\n\n\n\n\nQuality filtering with fastp\nSo far we have looked at the raw files and assessed their content, but we have not done anything about removing duplicates, sequences with low quality scores, or removal of the adaptors. So, let’s start this process.\nOur first step will be to perform quality filtering of the reads using a tool called fastp, which is versatile, easy to use, and fast.\n\n\n\n\n\n\nStepCreate directories that will store output files from the cleaning process\n\n\n\nmkdir -p /opt/results/cleaned/oral\n\n\nThe fastp command you will run contains multiple parameters, so let’s slowly deconstruct it:\n\n\n\n\n\n\nStepRun\n\n\n\nfastp --in1 /opt/data/oral_human_example_1_splitaa.fastq.gz \\\n      --in2 /opt/data/oral_human_example_2_splitaa.fastq.gz \\\n      --out1 /opt/results/cleaned/oral/oral_human_example_1_splitaa.trimmed.fastq.gz \\\n      --out2 /opt/results/cleaned/oral/oral_human_example_2_splitaa.trimmed.fastq.gz \\\n      -l 50 --cut_right --cut_right_window_size 4 --cut_right_mean_quality 20 -t 1 \\\n      --detect_adapter_for_pe \\\n      --json /opt/results/cleaned/oral/oral.fastp.json --html /opt/results/cleaned/oral/oral.fastp.html\n\n\n\n--in1/--in2 — The two input paired-end read files\n--out1/--out2 — The two output files after filtering\n-l — Minimum read length required, reads below 50 in this case are discarded\n--cut_right/--cut_right_window_size/--cut_right_mean_quality — These three options all work together. --cut_right creates a window, of size specified by --cut_right_window_size, which will slide from the front to the tail of the reads, calculating the mean quality score in the window at every step. If at any point, the mean quality score value is lower than the one specified by --cut_right_mean_quality, then the bases of that window and everyting to its right are immediately discarded for that read.\n-t 1 — Will trim the tail of its final base, as it’s a lot lower quality than other positions. This is a setting you should set very purposefully and for good reason, like we’re doing here.\n--detect_adapter_for_pe — One of the very useful features of fastp is that it can detect adapters automatically and remove them, which this parameter activates.\n--json/--html — Outputs a summary report similar to FastQC, in both .json and .html formats.\n\n\n\n\n\n\n\nQuestion\n\n\n\nFind and open the .html report. How many reads were removed? How has the average quality of the reads changed? \n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nComparing the total reads metric in the before and after sections of the report, we can see that about 3.9 million reads were removed completely. The average quality of the reads has already increased, which can be seen from comparing the Q20/Q30 bases before and after filtering.\n\n\n\n\n\nDecontamination with bowtie2\nNext, we want to remove any potential contamination in our reads, which we’ll do using a tool called bowtie2. It is always good to routinely screen for human DNA (which may come from the host and/or staff performing the experiment). However, if the sample is from a mouse, you would want to download the mouse genome. The first step in the decontamination process is therefore to make a database that our reads will be searched against for sources of contamination.\nIn the following exercise, we are going to use two “genomes” already downloaded for you in the decontamination folder. To make this tutorial quicker and smaller in terms of file sizes, we are going to use PhiX (a common spike-in) and just chromosome 10 from human.\n\n\n\n\n\n\nNoteThe reference sequences files we’ll be using\n\n\n\nls /opt/data/decontamination\n\n# Output: GRCh38_chr10.fasta  phix.fasta\n\n\nFor the next step, we need one file, so we want to merge the two different fasta files. This is simply done using the command-line tool cat.\n\n\n\n\n\n\nStepRun\n\n\n\ncat /opt/data/decontamination/GRCh38_chr10.fasta /opt/data/decontamination/phix.fasta &gt; /opt/data/decontamination/chr10_phix.fasta\n\n\nYou will often need to build indices for large sequence files - including sequencing files and reference files - to speed up computation. To build a bowtie index for our new concatenated PhiX-chr10 file, run the following script. NOTE. The indexing step can take a while to run (~ 2 -3 minutes for the example used in this practical)\n\n\n\n\n\n\nStepRun\n\n\n\nbowtie2-build /opt/data/decontamination/chr10_phix.fasta /opt/data/decontamination/chr10_phix.index\n\n#check output - indexed files end with *bt2\nls /opt/data/decontamination/chr10_phix.index*bt2\n\n# opt/data/decontamination/chr10_phix.index.1.bt2\n# opt/data/decontamination/chr10_phix.index.2.bt2\n# opt/data/decontamination/chr10_phix.index.3.bt2\n# opt/data/decontamination/chr10_phix.index.4.bt2\n# opt/data/decontamination/chr10_phix.index.rev.1.bt2\n# opt/data/decontamination/chr10_phix.index.rev.2.bt2\n\n\n\n\n\n\n\n\nNoteTip\n\n\n\nIt is possible to automatically download a pre-indexed human genome in bowtie2. Commonly-used bowtie2 indices can be downloaded from https://bowtie-bio.sourceforge.net/bowtie2/index.shtml.\n\n\nNow we are going to use our new indexed chr10_phix reference and decontaminate our already quality-filtered reads from fastp. Run bowtie2 as below. NOTE. This alignment step can take a few minutes to run.\n\n\n\n\n\n\nStepRun\n\n\n\nbowtie2 -1 /opt/results/cleaned/oral/oral_human_example_1_splitaa.trimmed.fastq.gz \\\n        -2 /opt/results/cleaned/oral/oral_human_example_2_splitaa.trimmed.fastq.gz \\\n        -x /opt/data/decontamination/chr10_phix.index \\\n        --un-conc-gz  /opt/results/cleaned/oral/oral_human_example.fastq.gz \\\n        --very-sensitive --dovetail &gt; /dev/null\n\n-1 - input read 1\n-2 - input read 2\n-x - reference genome index filename prefix (minus trailing .X.bt2)\n--un-con-gz - write pairs that didn’t align concordantly to assigned filepath (this will be your cleaned reads)\n--very-sensitive - set stringent parameters to call reads as a mapped read (Same as -D 20 -R 3 -N 0 -L 20 -i S,1,0.50)\n--dovetail - concordant when mates extend past each other (ie. the paired alignments overlaps one another )\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFrom the bowtie2 output on the terminal, what fraction of reads have been deemed to be contaminating?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nAs the bowtie2 output says, 3.15% reads have aligned to the reference, and are therefore contaminating.\n\n\n\nbowtie2 changes the naming scheme of the output files, so we rename them to be consistent:\n\n\n\n\n\n\nStepRun\n\n\n\nmv /opt/results/cleaned/oral/oral_human_example.fastq.1.gz /opt/results/cleaned/oral/oral_human_example_1_splitaa_trimmed_decontam.fastq.gz\nmv /opt/results/cleaned/oral/oral_human_example.fastq.2.gz /opt/results/cleaned/oral/oral_human_example_2_splitaa_trimmed_decontam.fastq.gz\n\n\n\n\nPost-QC assessment with FastQC and multiqc\n\n\n\n\n\n\nStepRun FastQC\n\n\n\nUsing what you have learned previously, generate a FastQC report for each of the *trimmed_decontam.fastq.gz files. Output the new fastqc report files in the same /opt/results/fastqc_results/oral directory as last time.\n\n\n\n\n\n\n\n\nCautionRun FastQC (code)\n\n\n\n\n\nfastqc /opt/results/cleaned/oral/oral_human_example_1_splitaa_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/oral\nfastqc /opt/results/cleaned/oral/oral_human_example_2_splitaa_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/oral\nchown 1001 /opt/results/fastqc_results/oral/*.html\n\n\n\n\n\n\n\n\n\nStepRun multiQC\n\n\n\nAlso generate a multiQC report, with /opt/results/fastqc_results/oral as input. The reason we generated the new FastQC reports in the same directory is so that you can compare how the reads have changed after the quality filtering and decontamination steps in the same final multiqc report.\nmkdir -p /opt/results/final_multiqc_results/oral\n&lt;you construct the command&gt;\n\n\n\n\n\n\n\n\nCautionRun multiQC (code)\n\n\n\n\n\nmkdir -p /opt/results/final_multiqc_results/oral\nmultiqc /opt/results/fastqc_results/oral -o /opt/results/final_multiqc_results/oral\n\n\n\n\n\n\n\n\n\nStepCheck report\n\n\n\nView the MultiQC report as before using your browser.\n\n\n\nScreenshot of multiQC\n\n\nScroll down through the report. The sequence quality histograms show the above results from each file as four separate lines. The ‘Status Checks’ show a matrix of which samples passed check and which ones have problems.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think of the change in sequence quality histograms? Have they improved?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe sequence quality histograms show clearly that the cleaned and decontaminated reads have improved quality throughout the reads now.\n\n\n\nThe reads have now been decontaminated and can be uploaded to ENA, one of the INSDC members. It is beyond the scope of this course to include a tutorial on how to submit to ENA, but there is additional information available on how to do this in this Online Training guide provided by EMBL-EBI",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#assembly-phix-decontamination",
    "href": "sessions/qc.html#assembly-phix-decontamination",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Assembly PhiX decontamination",
    "text": "Assembly PhiX decontamination\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\nIn the following exercises, you will generate a PhiX BLAST database and run a BLAST search with a subset of assembled freshwater sediment metagenomic reads to identify contamination.\n\n\nPhiX, used in the previous section of this practical, is a small bacteriophage genome typically used as a calibration control in sequencing runs. Most library preparations will use PhiX at low concentrations; however, it can still appear in the sequencing run. If not filtered out, PhiX can form small spurious contigs that could be incorrectly classified as diversity.\n\n\n\n\n\n\nStepGenerate the PhiX reference BLAST database:\n\n\n\nmakeblastdb -in /opt/data/decontamination/phix.fasta -input_type fasta -dbtype nucl -parse_seqids -out /opt/data/decontamination/phix_blastDB\n\n\nPrepare the freshwater sediment example assembly file and search against the new BLAST database. This assembly file contains only a subset of the contigs for the purpose of this practical.\n\n\n\n\n\n\nStepRun\n\n\n\nmkdir -p /opt/results/blast_results\ngunzip /opt/data/freshwater_sediment_contigs.fa.gz\nblastn -query /opt/data/freshwater_sediment_contigs.fa -db /opt/data/decontamination/phix_blastDB -task megablast -word_size 28 -best_hit_overhang 0.1 -best_hit_score_edge 0.1 -dust yes -evalue 0.0001 -min_raw_gapped_score 100 -penalty -5 -soft_masking true -window_size 100 -outfmt 6 -out /opt/results/blast_results/freshwater_blast_out.txt\n\n\nThe BLAST options are:\n\n-query — Input assembly fasta file.\n-out — Output file\n-db — Path to BLAST database.\n-task — Search type -“megablast”, for very similar sequences (e.g, sequencing errors)\n-word_size — Length of initial exact match\n\n\n\n\n\n\n\nStepAdd headers to the blast output and look at the contents of the final output file:\n\n\n\ncat /opt/data/blast_outfmt6.txt /opt/results/blast_results/freshwater_blast_out.txt &gt; /opt/results/blast_results/freshwater_blast_out_headers.txt\ncat /opt/results/blast_results/freshwater_blast_out_headers.txt\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre the hits significant?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe hits are very significant - they all have e-value hits of 0, which is the lowest it can go, and therefore the most confident the hit can be.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are the lengths of the matching contigs? We would typically filter metagenomic contigs at a length of 500bp. Would any PhiX contamination remain after this filter?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe lengths of the matching contigs, in order, are: 596, 426, 389, and 385. You’d therefore still have the longest contig still present using this filter, as it’s 596 bases long.\n\n\n\nNow that PhiX contamination was identified, it is important to remove these contigs from the assembly file before further analysis or upload to public archives. You can either remove these matching contigs directly, or use a tool like bowtie2 to achieve this like you learned in the last section.",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#using-controls",
    "href": "sessions/qc.html#using-controls",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Using Controls",
    "text": "Using Controls\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\nThis exercise will use 16S amplicon sequencing runs to look at the analysis of sequencing controls. Given two mystery sequencing runs, you will figure out which of the two is a control by comparing their microbial diversity, and also ascertain which kind of control it is.\n\n\n\nThe mystery sequencing runs\nGoing to /opt/data/mystery_reads, you will find two mystery FASTQ files. You have this information about them:\n\nThey are both amplicon sequencing runs.\nOne of the runs is a control\nOne of the runs is a mouse gut metagenome sequencing run of the 16S rRNA subunit.\n\nImagine you’re the one who sequenced these runs, and you want to analyse them, however you’ve forgotten which is the control. In this part of the practical, you will figure out which run was meant to be the control, and also the type of control it is, by using command-line tools to compare their microbial diversity.\nIn particular, you will use a tool called MAPseq and the 16S rRNA reference database SILVA to perform taxonomic classification of the sequencing reads. You will then be able to compare the assignments of the two sequencing runs to rediscover the purpose of each run.\n\n\n\n\n\n\nStepDownload the mystery sequencing runs into a new folder\n\n\n\nChange into the directory containing the mystery reads, and run ls -lh to see some information about the files.\ncd /opt/data/mystery_reads\nls -lh\n# total 7.7M\n# -rw-r--r-- 1 root root  26K Sep 11 12:24 mystery_reads_A.fastq.gz\n# -rw-r--r-- 1 root root 7.6M Sep 11 12:24 mystery_reads_B.fastq.gz\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCould you make an initial guess about which run is the control from this output?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe fifth column shows the file size, and it’s immediately noticeable how much smaller mystery_reads_A is compared to mystery_reads_B. This could indicate a lack of reads, which is either on purpose in the form of a control, or indicative of potential issues in sequencing.\n\n\n\n\n\nConverting FASTQ to FASTA with seqkit\nMAPseq requires the input reads to be in FASTA format rather than FASTQ. One very versatile and useful bioinformatics tool is SeqKit, which is a suite of commands for sequencing file manipulation.\n\n\n\n\n\n\nStepConvert FASTQ to FASTA with SeqKit\n\n\n\nThe SeqKit utility you want to use for the FASTQ-FASTA converstion is fq2fa. Go ahead and convert your FASTQ files like so\nseqkit fq2fa mystery_reads_A.fastq.gz -o mystery_reads_A.fasta.gz\nseqkit fq2fa mystery_reads_B.fastq.gz -o mystery_reads_B.fasta.gz\nYou will also need to uncompress the FASTA files before using MAPseq:\ngunzip mystery_reads_A.fasta.gz\ngunzip mystery_reads_B.fasta.gz\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThere’s another SeqKit utility that can generate summary metrics on the sequencing runs. Find it from the SeqKit documentation and use it on the mystery runs. What metric gives you another potential hint at the identity of the control?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe stats utility is very useful for getting a quick overview of sequence files:\nseqkit stats mystery_reads_*.fasta\n# file                      format  type  num_seqs     sum_len  min_len  avg_len  max_len\n# mystery_reads_A.fasta  FASTA   DNA        229      68,471      299      299      299\n# mystery_reads_B.fasta  FASTA   DNA     77,566  23,192,234      299      299      299\nThe num_seqs metric tells you how many sequences there are in each file. mystery_reads_A seems to have significantly less reads than mystery_reads_B, again hinting at it being a potential control, as you’d expect far more reads from an actual run.\n\n\n\n\n\nTaxonomic assignment of reads using MAPseq and SILVA\nNow that the reads are in the correct format, we can use MAPseq with the SILVA reference database to annotate the reads with taxonomic assignments.\n\n\n\n\n\n\nStepRun MAPseq on the reads\n\n\n\nThe SILVA reference database is already installed in the container at /SILVA-SSU/, so you can run MAPseq with the following commands:\nmapseq -seed 12 -tophits 80 -topotus 40 -outfmt simple mystery_reads_A.fasta /SILVA-SSU/SILVA-SSU.fasta /SILVA-SSU/SILVA-SSU-tax.txt &gt; mystery_reads_A.mseq\nmapseq -seed 12 -tophits 80 -topotus 40 -outfmt simple mystery_reads_B.fasta /SILVA-SSU/SILVA-SSU.fasta /SILVA-SSU/SILVA-SSU-tax.txt &gt; mystery_reads_B.mseq\nThis will generate two files of the .mseq format, which contain a taxonomy assignment for every read that had a match to the SILVA database. There is a lot of information about every matching read, such as matching coordinates, confidence, etc., but for our exercise we want to focus on the final column, which contains the actual assignment. Running these commands will aggregate that final column by computing a count of how many times each taxonomic assignment appears for both runs:\ntail -n +2 mystery_reads_A.mseq | cut -f14 | sort | uniq -c | awk '{print $1 \"\\t\" $2}' &gt; mystery_reads_A_counts.tsv\ntail -n +2 mystery_reads_B.mseq | cut -f14 | sort | uniq -c | awk '{print $1 \"\\t\" $2}' &gt; mystery_reads_B_counts.tsv\nFor example:\n23  sk__Bacteria;k__;p__Bacillota;c__Bacilli;o__Lactobacillales;f__Lactobacillaceae;g__Lactobacillus\nThis line means that this particular taxon was assigned to 23 different reads by MAPseq.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nOn your virtual machine, open the two counts files you’ve just generated side-by-side, and compare them. What do you notice about the amount of unique assignments in both files? How about the counts? What is your final conclusion about which run is the control, and the kind of control it is?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nIt is clear that mystery_reads_A has significantly less microbial diversity than mystery_reads_B. In terms of richness i.e. how many unique taxa there are, mystery_reads_A only has 29, while mystery_reads_B has 123. In terms of abundance i.e. the total counts, only 229 reads were given a taxonomic assignment in mystery_reads_A, while mystery_reads_B has a total read count of 77,565.\nFrom these observations, it becomes clear that mystery_reads_A is the control, and that it is specifically a negative control. Indeed, mystery_reads_A has the accession SRR17185965, and is a water negative control sample, while mystery_reads_B has the accession SRR17185970, and is a mouse gut metagenome sample.\nNow that you know it’s a negative control, do you identify any taxa the samples have in common? Can you make any assertions about what this means?",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#additional-exercise",
    "href": "sessions/qc.html#additional-exercise",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Additional Exercise",
    "text": "Additional Exercise\nIf you have finished the practical you can try this step for more practice assessing and trimming datasets, there is another set of raw reads called “skin_example_aa” from the skin metagenome available. These will require a FastQC or multiqc report, followed by quality filtering and mapping to the reference database with fastp and bowtie2. Using what you have learned previously, construct the relevant commands. Remember to check the quality before and after trimming.\n\n\n\n\n\n\nStepNavigate to skin folder and run quality control.\n\n\n\nls /opt/data/skin\n# Output: skin_example_aa_1.fastq.gz  skin_example_aa_2.fastq.gz  skin_neg_control.fastq.gz\n\n\nRemember you will need to run the following command to view any html files in the VM browsers:\nchown 1001 foldername/*.html\n\n\n\n\n\n\nCautionAdditional Excercise (code)\n\n\n\n\n\n#generate fastqc of raw reads of skin samples and negative control\nmkdir /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_example_aa_1.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_example_aa_2.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_neg_control.fastq.gz -o /opt/results/fastqc_results/skin\n\n#do quality filtering using fastp \nmkdir /opt/results/cleaned/skin\nfastp --in1 /opt/data/skin/skin_example_aa_1.fastq.gz \\\n      --in2 /opt/data/skin/skin_example_aa_2.fastq.gz \\\n      --out1 /opt/results/cleaned/skin/skin_example_aa_1.trimmed.fastq.gz \\\n      --out2 /opt/results/cleaned/skin/skin_example_aa_2.trimmed.fastq.gz \\\n      -l 50 --cut_right --cut_right_window_size 4 --cut_right_mean_quality 20 -t 1 \\\n      --detect_adapter_for_pe \\\n      --json /opt/results/cleaned/skin/oral.fastp.json --html /opt/results/cleaned/skin/skin.fastp.html\n\n#do host decontamination with bowtie2\nbowtie2 -1 /opt/results/cleaned/skin/skin_example_aa_1.trimmed.fastq.gz \\\n        -2 /opt/results/cleaned/skin/skin_example_aa_2.trimmed.fastq.gz \\\n        -x /opt/data/decontamination/chr10_phix.index \\\n        --un-conc-gz  /opt/results/cleaned/skin/skin_human_example.fastq.gz \\\n        --very-sensitive --dovetail &gt; /dev/null\n\n##decontaminated reads will be output as skin_human_example.fastq.1.gz and skin_human_example.fastq.2.gz in the /opt/results/cleaned/skin/ folder\n\n#rename decontaminated reads to be consistent\nmv /opt/results/cleaned/skin/skin_human_example.fastq.1.gz /opt/results/cleaned/skin/skin_human_example_1_trimmed_decontam.fastq.gz\nmv /opt/results/cleaned/skin/skin_human_example.fastq.2.gz /opt/results/cleaned/skin/skin_human_example_2_trimmed_decontam.fastq.gz\n\n#post-qc assessment with Fastqc and MultiQC\nfastqc /opt/results/cleaned/skin/skin_human_example_1_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/results/cleaned/skin/skin_human_example_2_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/skin\nchown 1001 /opt/results/fastqc_results/skin/*.html\n\n#generate MultiQC report of pre- and post-QC fastq files\nmultiqc /opt/results/fastqc_results/skin -o /opt/results/multiqc_results/skin\nchown 1001 /opt/results/fastqc_results/skin/*.html\n\n#visualise multiQC reports in web browser.",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#running-the-practical-externally",
    "href": "sessions/qc.html#running-the-practical-externally",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Running the practical externally",
    "text": "Running the practical externally\nWe need to first fetch the practical datasets.\nmkdir QC_session\ncd QC_session\n\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/qc-practical/qc_data.tar.gz\n# or\nrsync -av --partial --progress rsync://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/qc-practical/qc_data.tar.gz .\nOnce downloaded, extract the files from the tarball:\ntar -xzvf qc_data.tar.gz\n\nrm qc_data.tar.gz\n\nmkdir qc_results\n\ncd qc_data\nNow pull the docker container and export the above directories.\ndocker pull quay.io/microbiome-informatics/metagenomics-qc-practical\nexport DATADIR={path to quality directory}\nexport RESDIR={path to qc_results directory}\nYou will see the message “access control disabled, clients can connect from any host”\ndocker run --rm -it  -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/metagenomics-qc-practical\nThe container has the following tools installed:\n\nfastqc\nmultiqc\nfastp\nbowtie2\nblast\nseqkit\nmapseq\n\nYou can now continue this practical from the section “Quality control and filtering of the raw sequence files”",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/mags.html",
    "href": "sessions/mags.html",
    "title": "MAG Generation",
    "section": "",
    "text": "Generation of metagenome assembled genomes (MAGs) from assemblies\nAssessment of quality\nTaxonomic assignment\nVisualisation of taxonomy tree\n\n\n\nFor this tutorial, you will need to start the docker container by running the following command in the terminal:\nchmod a+rwx -R /home/training/course_dir/work_dir/Day_2/binning\ndocker pull quay.io/microbiome-informatics/mags-practical-2025:latest\ndocker run --rm --user 1001:1001 -it -v /home/training/course_dir/work_dir/Day_2/binning:/opt/data quay.io/microbiome-informatics/mags-practical-2025\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLearning Objectives - in the following exercises, you will learn how to bin an assembly, assess the quality of this assembly with СheckM and CheckM2, define GTDB and NCBI taxonomies and then visualize a placement of these genomes within a reference tree.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs with the assembly process, there are many software tools available for binning metagenome assemblies. Examples include, but are not limited to:\n\nMaxBin\nCONCOCT\nCOCACOLA\nMetaBAT\n\nThere is no clear winner between these tools, so it is best to experiment and compare a few different ones to determine which works best for your dataset. For this exercise, we will be using MetaBAT (specifically, MetaBAT2). The way in which MetaBAT bins contigs together is summarized in Figure 1.\n\n\n\n\n\nFigure 1. MetaBAT workflow (Kang, et al. PeerJ 2015).\n\n\n\n\n\n\n\n\n\n\nStep\n\n\n\nPrior to running MetaBAT, we need to generate coverage statistics by mapping reads to the contigs. To do this, we can use bwa and then the samtools software to reformat the output. This can take some time, so we have run it in advance.\n\n\n\n\n\n\n\n\nStep\n\n\n\nLet’s browse the files that we have prepared:\ncd /opt/data/assemblies/\nls\nYou should find the following files in this directory:\n\ncontigs.fasta: a file containing the primary metagenome assembly produced by metaSPAdes (contigs that haven’t been binned)\ninput.fastq.sam.bam: a pre-generated file that contains reads mapped back to contigs\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are very interested to generate the input.fastq.sam.bam file yourself, you can run the following commands (but better do it in the end of practical if you have time left):\n\n# NOTE: you will not be able to run subsequent steps until this workflow is completed because you need\n# the input.fastq.sam.bam file to calculate contig depth in the next step. In the interest of time, we\n# suggest that if you would like to try the commands, you run them after you complete the practical.\n\n# If you would like to practice generating the bam file, back up the input.fastq.sam.bam file that we\n# provided first, as these steps will take a while:\n\ncd /opt/data/assemblies/\nmv input.fastq.sam.bam input.fastq.sam.bam.bak\n\n# index the contigs file that was produced by metaSPAdes:\nbwa index contigs.fasta\n\n# fetch the reads from ENA\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_2.fastq.gz\n\n# map the original reads to the contigs:\nbwa mem contigs.fasta ERR011322_1.fastq.gz ERR011322_2.fastq.gz &gt; input.fastq.sam\n\n# reformat the file with samtools:\nsamtools view -Sbu input.fastq.sam &gt; junk \nsamtools sort junk -o input.fastq.sam.bam\n\n\n\n\n\n\n\n\n\n\n\nStepCreate a subdirectory where files will be saved to\n\n\n\ncd /opt/data/assemblies/\nmkdir contigs.fasta.metabat-bins2000\n\n\nIn this case, the directory might already be part of your VM, so do not worry if you get an error saying the directory already exists. You can move on to the next step.\n\n\n\n\n\n\nStep\n\n\n\nRun the following command to produce a contigs.fasta.depth.txt file, summarizing the output depth for use with MetaBAT:\njgi_summarize_bam_contig_depths --outputDepth contigs.fasta.depth.txt input.fastq.sam.bam\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow let’s put together the metaBAT2 command. To see the available options, run:\nmetabat2 -h\nHere is what we are trying to do:\n\nwe want to bin the assembly file called contigs.fasta\nthe resulting bins should be saved into the contigs.fasta.metabat-bins2000 folder\nwe want the bin file names to start with the prefix bin\nwe want to use the contig depth file we just generated (contigs.fasta.depth.txt)\nthe minimum contig length should be 2000\n\nTake a moment to put your command together but please check the answer below before running it to make sure everything is correct.\n\n\nSee the answer\n\nmetabat2 --inFile contigs.fasta --outFile contigs.fasta.metabat-bins2000/bin --abdFile contigs.fasta.depth.txt --minContig 2000\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce the binning process is complete, each bin will be grouped into a multi-fasta file with a name structure of **bin.[0-9]*.fa**.\n\n\n\n\n\n\n\n\nStep\n\n\n\nInspect the output of the binning process.\nls contigs.fasta.metabat-bins2000/bin*\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins did the process produce?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many sequences are in each bin?\n\n\n\n\n\n\nObviously, not all bins will have the same level of accuracy since some might represent a very small fraction of a potential species present in your dataset. To further assess the quality of the bins, we will use CheckM.\n\n\n\n\n\n\n\n\nNote\n\n\n\nCheckM has its own reference database of single-copy marker genes. Essentially, based on the proportion of these markers detected in the bin, the number of copies of each, and how different they are, it will determine the level of completeness, contamination, and strain heterogeneity of the predicted genome.\n\n\n\n\n\n\n\n\nStep\n\n\n\nBefore we start, we need to configure CheckM.\ncd /opt/data\nmkdir /opt/data/checkm_data\ntar -xf checkm_data.tar.gz -C /opt/data/checkm_data\ncheckm data setRoot /opt/data/checkm_data\n\n\nThis program has some handy tools not only for quality control but also for taxonomic classification, assessing coverage, building a phylogenetic tree, etc. The most relevant ones for this exercise are wrapped into the lineage_wf workflow.\n\n\n\n\n\n\nStep\n\n\n\nThis command uses a lot of memory. Do not run anything else while executing it.\ncd /opt/data/assemblies\ncheckm lineage_wf -x fa contigs.fasta.metabat-bins2000 checkm_output --tab_table -f MAGs_checkm.tab --reduced_tree -t 4\nDue to memory constraints (&lt; 40 GB), we have added the option --reduced_tree to build the phylogeny with a reduced number of reference genomes.\nOnce the lineage_wf analysis is done, the reference tree can be found in checkm_output/storage/tree/concatenated.tre.\nAdditionally, you will have the taxonomic assignment and quality assessment of each bin in the file MAGs_checkm.tab with the corresponding level of completeness, contamination, and strain heterogeneity (Fig. 2). A quick way to infer the overall quality of the bin is to calculate the level of **(completeness - 5*contamination). You should be aiming for an overall score of at least 70-80%**. We usually use 50% as the lowest acceptable cut-off (QS50)\nYou can inspect the CheckM output with:\ncat MAGs_checkm.tab\n\n\n\n\n\nToday researchers also use CheckM2, an improved method of predicting genome quality that uses machine learning. The execution of CheckM2 takes more time than CheckM. We have pre-generated the tab-delimited quality table with Checkm2 v1.1.0. It is available on our FTP.\n\n\n\n\n\n\nTip\n\n\n\nFYI command that was used to generate CheckM2 output:\ncheckm2 predict -i contigs.fasta.metabat-bins2000 -o checkm2_result --database_path CheckM2_database/uniref100.KO.1.dmnd -x fa --force -t 16\n\n\n\n\n\n\n\n\nStep\n\n\n\nDownload the CheckM2 result table that we have pre-generated:\n# create a folder for the CheckM2 result\ncd /opt/data/assemblies\nmkdir checkm2\ncd checkm2\n# download CheckM2 TSV result\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/checkm2_quality_report.tsv\n\n\nWe can compare CheckM and CheckM2 results using a scatter plot. We will plot completeness on the x-axis and contamination on the y-axis. We have created a simple python script to plot CheckM and CheckM2 results separately. Input table should be in the CSV format and it should contain 3 columns: bin name, completeness, and contamination. Required file header: “bin,completeness,contamination”\n\n\n\n\n\n\nStep\n\n\n\nModify the CheckM results table and generate a plot:\n# modify MAGs_checkm.tab\n# add header\necho \"bin,completeness,contamination\" &gt; checkm1_quality_report.csv\n# take the file without the header; leave only columns 1, 12 and 13; replace tabs with commas\ntail -n+2 MAGs_checkm.tab | cut -f1,12,13 | tr '\\t' ',' &gt;&gt; checkm1_quality_report.csv\n\n# plot\ncompleteness_vs_contamination.py -i checkm1_quality_report.csv -o checkm1\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow do the same for CheckM2. Modify the table and make a plot:\n# create CSV\n# add a header\necho \"bin,completeness,contamination\" &gt; checkm2_quality_report.csv\n# take the file without the header, leave only the first 3 columns, replace tabs with commas\ntail -n+2 checkm2_quality_report.tsv | cut -f1-3  | tr '\\t' ',' &gt;&gt; checkm2_quality_report.csv\n\n# plot\ncompleteness_vs_contamination.py -i checkm2_quality_report.csv -o checkm2\n\n\nYou should now have files checkm1.png and checkm2.png to compare the quality predictions between the two tools. To open the png files, use the file browser (grey folder icon in the left-hand side menu) and navigate to /home/training/course_dir/work_dir/Day_2/binning/assemblies\n\n\n\n\n\n\nQuestion\n\n\n\nDid CheckM and CheckM2 produce similar results? What can you say about the quality of the bins? How many bins pass QS50?\n\n\n\n\n\n\nUsually bins with bad quality are not included into further analysis. In MGnify we use criteria: 50 &lt;= completeness &lt;= 100 and 0 &lt;= contamination &lt;= 5 or more strict rule QS50: completeness - 5 x contamination &gt;= 50.\nIn our example, ideally we should exclude bin.1, bin.2 and bin.9 but we left those in the practical in order to see how different tools manage bad quality bins.\n\n\n\nAnother mandatory step that should be done in MAGs generation is dereplication of bins. This process includes identifying and removing redundant MAGs from a dataset by clustering bins based on their similarity, typically using Average Nucleotide Identity (ANI), and then selecting the highest-quality representative from each cluster. The most popular tool is dRep but, unfortunately, we do not have enough time to launch it during that practical. You can browse documentation of dRep if you are interested.\n\n\n\n\n\nA commonly used tool to determine genome taxonomy is GTDB-Tk. The taxonomy is built using phylogenomics (based on conserved single-copy marker genes across &gt;300,000 bacterial/archaeal genomes) and regularly updated. Due to the long time it takes to run it, we have already launched GTDB-Tk v2.4.1 with DB release v226 on all bins and saved the results to the FTP. Let’s take a look at the assigned taxonomy.\n\n\n\n\n\n\nTip\n\n\n\nFYI command that was used to generate GTDB-Tk output:\ngtdbtk classify_wf --cpus 16 --pplacer_cpus 8 --genome_dir contigs.fasta.metabat-bins2000 --extension fa --skip_ani_screen --out_dir gtdbtk_results\n\n\n\n\n\n\n\n\nStep\n\n\n\nDownload and check taxonomy file.\ncd /opt/data/assemblies\n\n# download the table\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/bins_v226_gtdbtk.bac120.summary.tsv\n\n# the first two columns of the table contain the bin name and the assigned taxonomy - take a look:\ncut -f1,2 bins_v226_gtdbtk.bac120.summary.tsv\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins were classified to the species level?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMake a guess why some bins were not assigned to any pylum?\n\n\n\n\n\nAnother widely used taxonomy is from NCBI but, unfortunately, GTDB and NCBI taxonomies are differ in their goals, methodology, and naming rules. NCBI taxonomy contains all organisms (bacteria, archaea, eukaryotes, viruses) but GTDB has only bacteria and archaea. Submission to NCBI is integrated with GenBank, RefSeq, SRA, and most bioinformatics pipelines. Taxonomy is built on historical taxonomic assignments (literature, culture collections) but it might be inconsistent (different ranks may not reflect phylogeny). Many bioinformatics tools and pipelines require NCBI taxID assigned for your data. It is possible to assign it with CAT or convert existing GTDB taxonomy into NCBI using script gtdb_to_ncbi_majority_vote.py provided by GTDB-Tk developers.\nAs we already have GTDB taxonomy lets convert it into NCBI. Converter input is full gtdb-tk output folder and metadata files for bacteria and archaea.\n\n\n\n\n\n\nStep\n\n\n\ncd /opt/data/assemblies\n\n# download the archive with gtdb-tk result\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/gtdbtk_results.tar.gz\n\n# uncompress archive\ntar -xzf gtdbtk_results.tar.gz\n\n# download metadata for archaea\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/ar53_metadata_r226.tsv\n\n# download metadata for bacteria\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/bac120_metadata_r226.tsv\n\n# run a converter\ngtdb_to_ncbi_majority_vote.py --gtdbtk_output_dir gtdbtk_results --output_file gtdb_ncbi_taxonomy.txt --ar53_metadata_file ar53_metadata_r226.tsv --bac120_metadata_file bac120_metadata_r226.tsv\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDo you find any differences in assigned taxonomies?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompare those lineages with Checkm results. What do you see?\n\n\n\n\n\nA taxonomy identifier (often called taxid) is a numeric identifier assigned to a taxon (species, genus, etc.) in a taxonomy database. It’s a stable way to refer to a taxon without relying on names, which can change. The simplest way to search for it is using Taxonomy Browser. You can also use ENA Text search.\n\n\n\n\n\n\nQuestion\n\n\n\nCan you determine TaxID for all bins (if the species-level TaxId is not known, a TaxId for a higher taxonomic level can be used)? Try to search for GTDB lineage. What can you see in taxonomy for bin.5?\n\n\n\n\n\nNow you can visualise your classified bins on a small bacterial phylogenetic tree with iTOL. A quick and user-friendly way to do this is to use the web-based interactive Tree of Life iTOL. To use iTOL you will need a user account. For the purpose of this tutorial we have already created one for you with an example tree. iTOL only takes in newick formatted trees.\nThat tree was generated with tool FastTree from a subset of sequences from multiple sequence alignment generated by GTDB-Tk tool. Alignment can be found in GTDB-Tk result folder gtdbtk_results/align/gtdbtk.bac120.msa.fasta.\n\n\n\n\n\n\nTip\n\n\n\nFYI commands that were used to generate the tree:\n# subsequence alignment file gtdbtk_results/align/gtdbtk.bac120.msa.fasta with seqkit tool to pick clades with bins\n# then build a tree\nFastTree -out small_bac_tree.nwk gtdbtk_results/align/gtdbtk.bac120.msa.chosen.fasta\n\n\n\n\n\n\n\n\nStep\n\n\n\n\nPre-download tree files from FTP:\n\n\ntree: https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/small_bac_tree.nwk\nlegend by phylum: https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/small_bac_tree.legend.txt\nlayers by phylum: https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/small_bac_tree.layer.txt\n\n\nRename files with prefix: your_name_training25.\nGo to the iTOL website (open the link in a new window).\nLogin is as follows:\nUser: EBI_training\nPassword: EBI_training\nAfter you login, just click on My Trees in the toolbar at the top\nFrom there select Tree upload and upload tree file with .nwk extension.\nOnce uploaded, click the tree name to visualize the plot\nTo colour the clades and the outside circle according to the phylum of each strain, drag and drop the files .legend.txt and .layer.txt\n\n\n\nNow you can play a bit with tree using Control panel on the left. For example, try to visualise Unrooted tree (Control panel -&gt; Basic -&gt; Mode -&gt; Unrooted).\nAs we built the tree from GTDB-Tk result we can easily assign taxonomy to tree nodes in iTOL. Go to: Control Panel -&gt; Advanced -&gt; Other functions -&gt; Auto assign taxonomy -&gt; GTDB.\n\n\n\n\n\n\nQuestion\n\n\n\nHow to find our bins on the tree? For example, try to find bin.7.\nHint: Use a magnifier with Aa sign on the left of the iTOL window and search for species name.\n\n\n\n\n\nOnce MAGs are recovered, annotation is a critical step in turning raw sequence data into biological knowledge. We have already done Taxonomic annotation.\nIt is also useful to have:\n\nStructural annotation – predicting genes, coding sequences (CDS), rRNAs, tRNAs, and other genomic features.\nFunctional annotation – assigning putative functions to predicted genes using homology searches against protein/domain databases (e.g., KEGG, Pfam, eggNOG, InterPro).\n\nThere are many pipelines available for MAGs annotation. MGnify team wildly uses pipeline mettannotator or you can also check nf-core mag pipeline.",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#prerequisites",
    "href": "sessions/mags.html#prerequisites",
    "title": "MAG Generation",
    "section": "",
    "text": "For this tutorial, you will need to start the docker container by running the following command in the terminal:\nchmod a+rwx -R /home/training/course_dir/work_dir/Day_2/binning\ndocker pull quay.io/microbiome-informatics/mags-practical-2025:latest\ndocker run --rm --user 1001:1001 -it -v /home/training/course_dir/work_dir/Day_2/binning:/opt/data quay.io/microbiome-informatics/mags-practical-2025",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#generating-metagenome-assembled-genomes",
    "href": "sessions/mags.html#generating-metagenome-assembled-genomes",
    "title": "MAG Generation",
    "section": "",
    "text": "Note\n\n\n\nLearning Objectives - in the following exercises, you will learn how to bin an assembly, assess the quality of this assembly with СheckM and CheckM2, define GTDB and NCBI taxonomies and then visualize a placement of these genomes within a reference tree.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs with the assembly process, there are many software tools available for binning metagenome assemblies. Examples include, but are not limited to:\n\nMaxBin\nCONCOCT\nCOCACOLA\nMetaBAT\n\nThere is no clear winner between these tools, so it is best to experiment and compare a few different ones to determine which works best for your dataset. For this exercise, we will be using MetaBAT (specifically, MetaBAT2). The way in which MetaBAT bins contigs together is summarized in Figure 1.\n\n\n\n\n\nFigure 1. MetaBAT workflow (Kang, et al. PeerJ 2015).\n\n\n\n\n\n\n\n\n\n\nStep\n\n\n\nPrior to running MetaBAT, we need to generate coverage statistics by mapping reads to the contigs. To do this, we can use bwa and then the samtools software to reformat the output. This can take some time, so we have run it in advance.\n\n\n\n\n\n\n\n\nStep\n\n\n\nLet’s browse the files that we have prepared:\ncd /opt/data/assemblies/\nls\nYou should find the following files in this directory:\n\ncontigs.fasta: a file containing the primary metagenome assembly produced by metaSPAdes (contigs that haven’t been binned)\ninput.fastq.sam.bam: a pre-generated file that contains reads mapped back to contigs\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are very interested to generate the input.fastq.sam.bam file yourself, you can run the following commands (but better do it in the end of practical if you have time left):\n\n# NOTE: you will not be able to run subsequent steps until this workflow is completed because you need\n# the input.fastq.sam.bam file to calculate contig depth in the next step. In the interest of time, we\n# suggest that if you would like to try the commands, you run them after you complete the practical.\n\n# If you would like to practice generating the bam file, back up the input.fastq.sam.bam file that we\n# provided first, as these steps will take a while:\n\ncd /opt/data/assemblies/\nmv input.fastq.sam.bam input.fastq.sam.bam.bak\n\n# index the contigs file that was produced by metaSPAdes:\nbwa index contigs.fasta\n\n# fetch the reads from ENA\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_2.fastq.gz\n\n# map the original reads to the contigs:\nbwa mem contigs.fasta ERR011322_1.fastq.gz ERR011322_2.fastq.gz &gt; input.fastq.sam\n\n# reformat the file with samtools:\nsamtools view -Sbu input.fastq.sam &gt; junk \nsamtools sort junk -o input.fastq.sam.bam\n\n\n\n\n\n\n\n\n\n\n\nStepCreate a subdirectory where files will be saved to\n\n\n\ncd /opt/data/assemblies/\nmkdir contigs.fasta.metabat-bins2000\n\n\nIn this case, the directory might already be part of your VM, so do not worry if you get an error saying the directory already exists. You can move on to the next step.\n\n\n\n\n\n\nStep\n\n\n\nRun the following command to produce a contigs.fasta.depth.txt file, summarizing the output depth for use with MetaBAT:\njgi_summarize_bam_contig_depths --outputDepth contigs.fasta.depth.txt input.fastq.sam.bam\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow let’s put together the metaBAT2 command. To see the available options, run:\nmetabat2 -h\nHere is what we are trying to do:\n\nwe want to bin the assembly file called contigs.fasta\nthe resulting bins should be saved into the contigs.fasta.metabat-bins2000 folder\nwe want the bin file names to start with the prefix bin\nwe want to use the contig depth file we just generated (contigs.fasta.depth.txt)\nthe minimum contig length should be 2000\n\nTake a moment to put your command together but please check the answer below before running it to make sure everything is correct.\n\n\nSee the answer\n\nmetabat2 --inFile contigs.fasta --outFile contigs.fasta.metabat-bins2000/bin --abdFile contigs.fasta.depth.txt --minContig 2000\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce the binning process is complete, each bin will be grouped into a multi-fasta file with a name structure of **bin.[0-9]*.fa**.\n\n\n\n\n\n\n\n\nStep\n\n\n\nInspect the output of the binning process.\nls contigs.fasta.metabat-bins2000/bin*\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins did the process produce?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many sequences are in each bin?",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#quality-assessment",
    "href": "sessions/mags.html#quality-assessment",
    "title": "MAG Generation",
    "section": "",
    "text": "Obviously, not all bins will have the same level of accuracy since some might represent a very small fraction of a potential species present in your dataset. To further assess the quality of the bins, we will use CheckM.\n\n\n\n\n\n\n\n\nNote\n\n\n\nCheckM has its own reference database of single-copy marker genes. Essentially, based on the proportion of these markers detected in the bin, the number of copies of each, and how different they are, it will determine the level of completeness, contamination, and strain heterogeneity of the predicted genome.\n\n\n\n\n\n\n\n\nStep\n\n\n\nBefore we start, we need to configure CheckM.\ncd /opt/data\nmkdir /opt/data/checkm_data\ntar -xf checkm_data.tar.gz -C /opt/data/checkm_data\ncheckm data setRoot /opt/data/checkm_data\n\n\nThis program has some handy tools not only for quality control but also for taxonomic classification, assessing coverage, building a phylogenetic tree, etc. The most relevant ones for this exercise are wrapped into the lineage_wf workflow.\n\n\n\n\n\n\nStep\n\n\n\nThis command uses a lot of memory. Do not run anything else while executing it.\ncd /opt/data/assemblies\ncheckm lineage_wf -x fa contigs.fasta.metabat-bins2000 checkm_output --tab_table -f MAGs_checkm.tab --reduced_tree -t 4\nDue to memory constraints (&lt; 40 GB), we have added the option --reduced_tree to build the phylogeny with a reduced number of reference genomes.\nOnce the lineage_wf analysis is done, the reference tree can be found in checkm_output/storage/tree/concatenated.tre.\nAdditionally, you will have the taxonomic assignment and quality assessment of each bin in the file MAGs_checkm.tab with the corresponding level of completeness, contamination, and strain heterogeneity (Fig. 2). A quick way to infer the overall quality of the bin is to calculate the level of **(completeness - 5*contamination). You should be aiming for an overall score of at least 70-80%**. We usually use 50% as the lowest acceptable cut-off (QS50)\nYou can inspect the CheckM output with:\ncat MAGs_checkm.tab\n\n\n\n\n\nToday researchers also use CheckM2, an improved method of predicting genome quality that uses machine learning. The execution of CheckM2 takes more time than CheckM. We have pre-generated the tab-delimited quality table with Checkm2 v1.1.0. It is available on our FTP.\n\n\n\n\n\n\nTip\n\n\n\nFYI command that was used to generate CheckM2 output:\ncheckm2 predict -i contigs.fasta.metabat-bins2000 -o checkm2_result --database_path CheckM2_database/uniref100.KO.1.dmnd -x fa --force -t 16\n\n\n\n\n\n\n\n\nStep\n\n\n\nDownload the CheckM2 result table that we have pre-generated:\n# create a folder for the CheckM2 result\ncd /opt/data/assemblies\nmkdir checkm2\ncd checkm2\n# download CheckM2 TSV result\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/checkm2_quality_report.tsv\n\n\nWe can compare CheckM and CheckM2 results using a scatter plot. We will plot completeness on the x-axis and contamination on the y-axis. We have created a simple python script to plot CheckM and CheckM2 results separately. Input table should be in the CSV format and it should contain 3 columns: bin name, completeness, and contamination. Required file header: “bin,completeness,contamination”\n\n\n\n\n\n\nStep\n\n\n\nModify the CheckM results table and generate a plot:\n# modify MAGs_checkm.tab\n# add header\necho \"bin,completeness,contamination\" &gt; checkm1_quality_report.csv\n# take the file without the header; leave only columns 1, 12 and 13; replace tabs with commas\ntail -n+2 MAGs_checkm.tab | cut -f1,12,13 | tr '\\t' ',' &gt;&gt; checkm1_quality_report.csv\n\n# plot\ncompleteness_vs_contamination.py -i checkm1_quality_report.csv -o checkm1\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow do the same for CheckM2. Modify the table and make a plot:\n# create CSV\n# add a header\necho \"bin,completeness,contamination\" &gt; checkm2_quality_report.csv\n# take the file without the header, leave only the first 3 columns, replace tabs with commas\ntail -n+2 checkm2_quality_report.tsv | cut -f1-3  | tr '\\t' ',' &gt;&gt; checkm2_quality_report.csv\n\n# plot\ncompleteness_vs_contamination.py -i checkm2_quality_report.csv -o checkm2\n\n\nYou should now have files checkm1.png and checkm2.png to compare the quality predictions between the two tools. To open the png files, use the file browser (grey folder icon in the left-hand side menu) and navigate to /home/training/course_dir/work_dir/Day_2/binning/assemblies\n\n\n\n\n\n\nQuestion\n\n\n\nDid CheckM and CheckM2 produce similar results? What can you say about the quality of the bins? How many bins pass QS50?",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#quality-filtering",
    "href": "sessions/mags.html#quality-filtering",
    "title": "MAG Generation",
    "section": "",
    "text": "Usually bins with bad quality are not included into further analysis. In MGnify we use criteria: 50 &lt;= completeness &lt;= 100 and 0 &lt;= contamination &lt;= 5 or more strict rule QS50: completeness - 5 x contamination &gt;= 50.\nIn our example, ideally we should exclude bin.1, bin.2 and bin.9 but we left those in the practical in order to see how different tools manage bad quality bins.",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#dereplication",
    "href": "sessions/mags.html#dereplication",
    "title": "MAG Generation",
    "section": "",
    "text": "Another mandatory step that should be done in MAGs generation is dereplication of bins. This process includes identifying and removing redundant MAGs from a dataset by clustering bins based on their similarity, typically using Average Nucleotide Identity (ANI), and then selecting the highest-quality representative from each cluster. The most popular tool is dRep but, unfortunately, we do not have enough time to launch it during that practical. You can browse documentation of dRep if you are interested.",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#taxonomy-assignment",
    "href": "sessions/mags.html#taxonomy-assignment",
    "title": "MAG Generation",
    "section": "",
    "text": "A commonly used tool to determine genome taxonomy is GTDB-Tk. The taxonomy is built using phylogenomics (based on conserved single-copy marker genes across &gt;300,000 bacterial/archaeal genomes) and regularly updated. Due to the long time it takes to run it, we have already launched GTDB-Tk v2.4.1 with DB release v226 on all bins and saved the results to the FTP. Let’s take a look at the assigned taxonomy.\n\n\n\n\n\n\nTip\n\n\n\nFYI command that was used to generate GTDB-Tk output:\ngtdbtk classify_wf --cpus 16 --pplacer_cpus 8 --genome_dir contigs.fasta.metabat-bins2000 --extension fa --skip_ani_screen --out_dir gtdbtk_results\n\n\n\n\n\n\n\n\nStep\n\n\n\nDownload and check taxonomy file.\ncd /opt/data/assemblies\n\n# download the table\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/bins_v226_gtdbtk.bac120.summary.tsv\n\n# the first two columns of the table contain the bin name and the assigned taxonomy - take a look:\ncut -f1,2 bins_v226_gtdbtk.bac120.summary.tsv\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins were classified to the species level?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMake a guess why some bins were not assigned to any pylum?\n\n\n\n\n\nAnother widely used taxonomy is from NCBI but, unfortunately, GTDB and NCBI taxonomies are differ in their goals, methodology, and naming rules. NCBI taxonomy contains all organisms (bacteria, archaea, eukaryotes, viruses) but GTDB has only bacteria and archaea. Submission to NCBI is integrated with GenBank, RefSeq, SRA, and most bioinformatics pipelines. Taxonomy is built on historical taxonomic assignments (literature, culture collections) but it might be inconsistent (different ranks may not reflect phylogeny). Many bioinformatics tools and pipelines require NCBI taxID assigned for your data. It is possible to assign it with CAT or convert existing GTDB taxonomy into NCBI using script gtdb_to_ncbi_majority_vote.py provided by GTDB-Tk developers.\nAs we already have GTDB taxonomy lets convert it into NCBI. Converter input is full gtdb-tk output folder and metadata files for bacteria and archaea.\n\n\n\n\n\n\nStep\n\n\n\ncd /opt/data/assemblies\n\n# download the archive with gtdb-tk result\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/gtdbtk_results.tar.gz\n\n# uncompress archive\ntar -xzf gtdbtk_results.tar.gz\n\n# download metadata for archaea\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/ar53_metadata_r226.tsv\n\n# download metadata for bacteria\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/bac120_metadata_r226.tsv\n\n# run a converter\ngtdb_to_ncbi_majority_vote.py --gtdbtk_output_dir gtdbtk_results --output_file gtdb_ncbi_taxonomy.txt --ar53_metadata_file ar53_metadata_r226.tsv --bac120_metadata_file bac120_metadata_r226.tsv\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDo you find any differences in assigned taxonomies?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompare those lineages with Checkm results. What do you see?\n\n\n\n\n\nA taxonomy identifier (often called taxid) is a numeric identifier assigned to a taxon (species, genus, etc.) in a taxonomy database. It’s a stable way to refer to a taxon without relying on names, which can change. The simplest way to search for it is using Taxonomy Browser. You can also use ENA Text search.\n\n\n\n\n\n\nQuestion\n\n\n\nCan you determine TaxID for all bins (if the species-level TaxId is not known, a TaxId for a higher taxonomic level can be used)? Try to search for GTDB lineage. What can you see in taxonomy for bin.5?\n\n\n\n\n\nNow you can visualise your classified bins on a small bacterial phylogenetic tree with iTOL. A quick and user-friendly way to do this is to use the web-based interactive Tree of Life iTOL. To use iTOL you will need a user account. For the purpose of this tutorial we have already created one for you with an example tree. iTOL only takes in newick formatted trees.\nThat tree was generated with tool FastTree from a subset of sequences from multiple sequence alignment generated by GTDB-Tk tool. Alignment can be found in GTDB-Tk result folder gtdbtk_results/align/gtdbtk.bac120.msa.fasta.\n\n\n\n\n\n\nTip\n\n\n\nFYI commands that were used to generate the tree:\n# subsequence alignment file gtdbtk_results/align/gtdbtk.bac120.msa.fasta with seqkit tool to pick clades with bins\n# then build a tree\nFastTree -out small_bac_tree.nwk gtdbtk_results/align/gtdbtk.bac120.msa.chosen.fasta\n\n\n\n\n\n\n\n\nStep\n\n\n\n\nPre-download tree files from FTP:\n\n\ntree: https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/small_bac_tree.nwk\nlegend by phylum: https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/small_bac_tree.legend.txt\nlayers by phylum: https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2025/mags/taxonomy/small_bac_tree.layer.txt\n\n\nRename files with prefix: your_name_training25.\nGo to the iTOL website (open the link in a new window).\nLogin is as follows:\nUser: EBI_training\nPassword: EBI_training\nAfter you login, just click on My Trees in the toolbar at the top\nFrom there select Tree upload and upload tree file with .nwk extension.\nOnce uploaded, click the tree name to visualize the plot\nTo colour the clades and the outside circle according to the phylum of each strain, drag and drop the files .legend.txt and .layer.txt\n\n\n\nNow you can play a bit with tree using Control panel on the left. For example, try to visualise Unrooted tree (Control panel -&gt; Basic -&gt; Mode -&gt; Unrooted).\nAs we built the tree from GTDB-Tk result we can easily assign taxonomy to tree nodes in iTOL. Go to: Control Panel -&gt; Advanced -&gt; Other functions -&gt; Auto assign taxonomy -&gt; GTDB.\n\n\n\n\n\n\nQuestion\n\n\n\nHow to find our bins on the tree? For example, try to find bin.7.\nHint: Use a magnifier with Aa sign on the left of the iTOL window and search for species name.\n\n\n\n\n\nOnce MAGs are recovered, annotation is a critical step in turning raw sequence data into biological knowledge. We have already done Taxonomic annotation.\nIt is also useful to have:\n\nStructural annotation – predicting genes, coding sequences (CDS), rRNAs, tRNAs, and other genomic features.\nFunctional annotation – assigning putative functions to predicted genes using homology searches against protein/domain databases (e.g., KEGG, Pfam, eggNOG, InterPro).\n\nThere are many pipelines available for MAGs annotation. MGnify team wildly uses pipeline mettannotator or you can also check nf-core mag pipeline.",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/getting_data_from_mgnify.html",
    "href": "sessions/getting_data_from_mgnify.html",
    "title": "Getting data from MGnify",
    "section": "",
    "text": "Exploring MGnify Studies\nExploring MGnify Analyses\nMetagenomic Assembled Genomes\n\n\n\nFor this tutorial, you will need to start the predefined Jupyter notebook and complete the exercises within it. follow the steps below To start the notebook.\n\nOpen this link on your browser: https://sa.ndyroge.rs/mgnify-tutorial/notebooks/index.html?path=tutorial.ipynb\nClick on the run menu, then click run all cells\nGo through the exercises in the notebook\n\nNOTE: In regards to Task 3.1, please begin from page 61 of the results\n\n\n\nA new marine MAG catalogue was recently published on MGnify. When you get to task 3.3, make use of this MAG linked below: MGYG000445242\nIf you’re interested in learning more about the new marine MAG catalogue, you can find more information here",
    "crumbs": [
      "Home",
      "Sessions",
      "Getting data from MGnify"
    ]
  },
  {
    "objectID": "sessions/getting_data_from_mgnify.html#instructions",
    "href": "sessions/getting_data_from_mgnify.html#instructions",
    "title": "Getting data from MGnify",
    "section": "",
    "text": "For this tutorial, you will need to start the predefined Jupyter notebook and complete the exercises within it. follow the steps below To start the notebook.\n\nOpen this link on your browser: https://sa.ndyroge.rs/mgnify-tutorial/notebooks/index.html?path=tutorial.ipynb\nClick on the run menu, then click run all cells\nGo through the exercises in the notebook\n\nNOTE: In regards to Task 3.1, please begin from page 61 of the results",
    "crumbs": [
      "Home",
      "Sessions",
      "Getting data from MGnify"
    ]
  },
  {
    "objectID": "sessions/getting_data_from_mgnify.html#note-in-regards-to-task-3.3-take-note-of-the-following",
    "href": "sessions/getting_data_from_mgnify.html#note-in-regards-to-task-3.3-take-note-of-the-following",
    "title": "Getting data from MGnify",
    "section": "",
    "text": "A new marine MAG catalogue was recently published on MGnify. When you get to task 3.3, make use of this MAG linked below: MGYG000445242\nIf you’re interested in learning more about the new marine MAG catalogue, you can find more information here",
    "crumbs": [
      "Home",
      "Sessions",
      "Getting data from MGnify"
    ]
  },
  {
    "objectID": "sessions/assemblies.html",
    "href": "sessions/assemblies.html",
    "title": "Assembly and Co-assembly of Metagenomic Raw Reads",
    "section": "",
    "text": "Learning Objectives\nIn the following exercises, you will learn how to perform metagenomic assembly and co-assembly, and explore the output. We will visualize assembly graphs with Bandage, examine assembly statistics with assembly_stats, and align contigs against the BLAST database.\n\n\n\n\n\n\nNote\n\n\n\nMetagenomic assembly can take hours or even days to complete on real samples, often requiring days of CPU time and hundreds of gigabytes of memory. In this practical, we will work with small toy datasets to demonstrate the assembly process.\n\n\nOnce you have quality-filtered your sequencing reads, you may want to perform de novo assembly in addition to, or as an alternative to, read-based analyses. The first step is to assemble your sequences into contigs. There are many tools available for this purpose, such as metaSPAdes, IDBA-UD, metaMDBG, and MEGAHIT.\nWe generally use metaSPAdes, as it typically yields the best contig size statistics (i.e., more contiguous assembly) and has been shown to capture high degrees of community diversity (Vollmers, et al. PLOS One 2017). However, you should consider the pros and cons of different assemblers based on your specific needs. Key factors include not only assembly accuracy but also computational overhead, which varies significantly between tools.\nFor example, metaSPAdes requires substantial memory, especially for very diverse samples with large amounts of sequence data (e.g., soil samples). In such cases, more memory-efficient alternatives like MEGAHIT may be preferable.\nIn the following practicals, we will demonstrate the use of metaSPAdes on a small short-read sample, Flye on a long-read sample, and MEGAHIT for co-assembly.\n\n\nBefore we start…\nFirst, let’s navigate to the root working directory where we will run all analyses:\ncd /home/training/Assembly/\nThe raw reads for assembly generation can be found in the data_dir/short_reads, data_dir/long_reads and data_dir/co_assembly_short_reads folders.\n\n\nShort-read assemblies: metaSPAdes\nFor short reads, we will use SPAdes - St. Petersburg genome Assembler, a suite of assembly tools designed for different types of sequencing data. For metagenomic data, we will use the metagenomic-specific mode of SPAdes called metaSPAdes.\nmetaSPAdes offers many options to fit different requirements, which depend primarily on the properties of data you want to assemble and desired result. Keep in mind that options differ between tools (e.g., spades.py vs metaspades.py).\n\n\n\n\n\n\nStep\n\n\n\nTo explore available options of metaSPAdes, type:\nmetaspades.py -h\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat metaSPAdes command should be used for the case described below?\nYou need to assemble paired-end reads from the short_reads folder and save the output to a folder named assembly_spades. Limit resource usage to 5 GB of memory and 4 threads. By default, metaSPAdes performs an error correction on the input reads before assembling. Since provided reads have already been polished, you can run the assembler without the error correction step.\nReview the help output from metaspades.py -h and select the appropriate options to build your first assembly command.\n\n\n\n\nClick to see solution\n\nThe correct metaSPAdes parameters are:\n\n-1 short_reads/input_1.fastq: path to forward reads\n-2 short_reads/input_2.fastq: path to reverse reads\n-o assembly_spades: output folder name\n-t 4: use 4 threads\n-m 5: limit memory usage to 5 Gb\n--only-assembler: skips the error correction step\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease do not execute the assembly command on your machine. Execution time would overall cover half of the session (metaSPAdes alone would take ~15 minutes). Moreover, the VMs have limited resources that barely meet metaSPAdes’ requirements, and running multiple assemblies simultaneously could overload the system.\n\n\nAll output files (including intermediate ones) can be found in the assembly_spades folder.\ncontigs.fasta and scaffolds.fasta are typically used for downstream analyses (e.g., binning and MAG generation). We will focus on contigs.fasta for this session, which is the same file you will use in the upcoming practicals.\nWithout going all the way to MAG generation, you can sometimes identify strong taxonomic signals at the assembly stage using a quick blastn alignment.\n\n\n\n\n\n\nStep\n\n\n\nOpen blast website and choose Nucleotide BLAST (blastn). Take the first 100 lines of the sequence by copying the output of the command below:\nhead -n 101 assembly_spades/contigs.fasta\nLeave all other options as default on the search page and run BLAST search. The resulting output will have the following format (but will look a bit better than this): \n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBased on the top BLAST hits, what species does this contig appear to originate from?\nThis read dataset was sequenced from a human oral sample. Is this organism commonly found in the human oral microbiome?\n\n\nAs mentioned in the theory session, you will need different statistics to evaluate the quality of your assembly.\nassembly_stats is a tool that produces two simple tables in JSON format with various measures, including N10 to N50, GC content, longest contig length, and more. The first section of the JSON output corresponds to scaffolds, while the second corresponds to contigs.\nN50 is a metric used to describe the quality of assembled genomes that are fragmented in contigs of different length. It represents the sequence length of the shortest contig at 50% of the total assembly length (when contigs are sorted from longest to shortest).\nA (hopefully) clarifying picture to understand N50, where N50==60: \nEssentially, a higher N50 value indicates a better assembly, as it means that longer contigs cover half of the final assembly, making it less fragmented. However, keep in mind that using N50 to compare the quality of two metagenomes only makes sense if the metagenomes are similar.\nNote that other metrics follow the same principle: for example, N90 represents the shortest contig length needed to cover 90% of the metagenome.\n\n\n\n\n\n\nStep\n\n\n\nYou can run assembly_stats with the following command:\nassembly_stats assembly_spades/scaffolds.fasta\n\n\nYou will see a summary output with statistics for your assembly. In lines with the format N50 = YYY, n = Z, the value n represents the number of sequences needed to cover 50% of the total assembly length. A “gap” is any consecutive run of Ns (undetermined nucleotide bases) of any length. N_count is the total number of Ns across the entire assembly.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are the lengths of the longest and shortest contigs?\nWhat is the N50 of the assembly? Considering that the input consisted of ~150 bp paired-end reads, what do these statistics tell you about the quality of the assembly?\n\n\nAnother useful tool for assessing metagenomic assemblies is QUAST, which provides deeper insights into assembly statistics — such as indel frequency and misassembly rate — in just a short amount of time.\n\n\nLong-read assemblies: Flye\nFor long reads, we will use Flye, an assembler designed for single-molecule sequencing reads such as those produced by PacBio and Oxford Nanopore Technologies (ONT). Like SPAdes, Flye performs both reads error correction and assembly. It compensates for the high base-calling error rate in long reads by comparing multiple reads that cover the same genomic region.\n\n\n\n\n\n\nStep\n\n\n\nYou can view all Flye parameters using the help command:\nflye -h\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat Flye command should be used for the case described below?\nYou need to assemble ONT reads from the long_reads folder using 4 threads and save the output to a folder named assembly_flye_meta. The provided ONT reads are raw and require error correction. Also, keep in mind that by default, Flye performs an isolate assembly, but in this case, we want to perform a metagenomic assembly.\n\n\n\n\nClick to see solution\n\nThe correct Flye parameters are:\n\n--nano-raw long_reads/ONT_input.fastq: assemble input reads with error correction. Depending on data quality, other presets can be used. For example, if reads have been pre-polished and adapters removed, use --nano-corr. The --nano-hq option should be reserved for high-quality data. The same logic applies to the pacbio options.\n--threads 4: number of threads\n--out-dir assembly_flye_meta: output folder\n--meta: enable metagenome mode\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs before, we recommend to NOT launch this command during this session. Each run takes around 5 minutes and would fully occupy all available CPU cores.\n\n\nThe output files generated by Flye can be found in the assembly_flye_meta folder.\n\n\n\n\n\n\nNote\n\n\n\nTo further investigate the effect of the --meta flag, we also generated another assembly without it. The output of that run can be found in the assembly_flye folder.\n\n\n\n\nDiving Into Assembly Graphs\nLet’s take a first look at what assembly graphs actually look like. Bandage (short for Bioinformatics Application for Navigating De novo Assembly Graphs Easily) is a program that creates interactive visualisations of assembly graphs. It can be used to explore different parts of the graph — for example, rRNA genes, SNPs, or inspect specific genomic regions. With it, you can zoom and pan around the graph, search for sequences, and perform many other operations.\nIn this exercise, we will use Bandage to compare the assembly graphs we generated with Flye and metaSPAdes. We will start with metaSPAdes.\nWhen working with metaSPAdes output, it is usually recommended to open the file assembly_graph.fastg in Bandage. However, our assembly is quite fragmented, so in this case we will load assembly_graph_after_simplification.gfa instead.\n\n\n\n\n\n\nStep\n\n\n\nExecute the following command to lauch the Bandage application:\nBandage &\nIn the Bandage graphical interface, perform the following steps:\n\nGo to File → Load graph\nNavigate to /home/training/Assembly/assembly_spades\nSelect and open the file assembly_graph_after_simplification.gfa\n\nOnce the file is loaded, you need to draw the graph. To do so, in the Graph drawing panel on the left side perform the following:\n\nSet Scope to Entire graph\nClick on Draw graph\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you identify any large or complex regions within the metaSPAdes graph? If so, what do they look like?\n\n\nNow, open another instance of Bandage as you did before, and load assembly_flye/assembly_graph.gfa.\n\n\n\n\n\n\nQuestion\n\n\n\nHow does the Flye assembly differ from the one generated with metaSPAdes?\n\n\nAs mentioned earlier, we ran Flye both with and without the --meta flag on long_reads/ONT_input.fastq. You can now repeat the same visualization procedure for the other Flye graph and compare the results.\nNotice how the metagenomic assembly mode affects the graph structure compared to the default isolate assembly mode.\n\n\n\n\n\n\nQuestion\n\n\n\nThis ONT dataset originates from run ERR3775163, which can be explored on the NCBI browser. Take a look at its metadata.\nCan you figure out why the assembly graph from the run without --meta looks better than the one with --meta?\n\n\n\n\n\n\n\n\nNoteExtra\n\n\n\nTry running a BLAST search for the first 100 lines of the first contig of the long-read assembly. (You won’t be able to BLAST the entire contigs as they are too long.) Do the results match the metadata you found on ENA?\n\n\n\n\nCo-assemblies: MEGAHIT\nIn the next part of this exercise, we will explore co-assembly of multiple datasets. Remember that co-assembly produces meaningful results only when applied to similar samples. This is true for the raw reads we co-assembled here — they all originate from a single sample that was split for this exercise.\n\n\n\n\n\n\nStep\n\n\n\nExplore MEGAHIT parameters using the help command:\nmegahit -h\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs before, we recommend not to launch MEGAHIT command this time.\n\n\nYou will find MEGAHIT output files in the co_assembly_short_reads folder.\nThe following parameters were used to generate the co-assemblies:\n\n-1 [forward files comma-separated]\n-2 [reverse files comma-separated]\n-o co_assembly_megahit output folder\n-t 4 number of threads\n--k-list 23,51,77 list of k-mer lengths\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompare the co-assembly contigs file with the single-assembly one using the assembly_stats tool.\nHow does this assembly differ from the one generated earlier with metaSPAdes? Which one do you think is better, and why?\n\n\n\n\n\n\n\n\nNoteExtra\n\n\n\nYou might notice that MEGAHIT does not generate assembly graphs by default. To create one, run the following command:\nmegahit_toolkit contig2fastg 77 co_assembly_megahit/final.contigs.fa &gt; co_assembly_megahit/final.contigs.fastg\nExplore the graph in Bandage.\nThe datasets used for the metaSPAdes assembly and the MEGAHIT co-assembly originate from the same source. Do you observe any relevant differences between the two assembly graphs?\n\n\n\n\nThe End\nFantastic work — you’ve reached the end of the practical! 🎉\nYou’ve explored different assemblers, compared their outputs, and learned how to visualise and interpret assembly graphs — great progress toward mastering metagenomics.\nTake a moment to reflect on what you’ve learned today.\nIf anything remains unclear, don’t hesitate to ask questions — there are no silly questions!\nIf you still have some spare time, take a look at the sections labelled “Extra.” They contain optional exercises for curious students who want to explore further.\n\n\n\n\nReuseApache 2.0",
    "crumbs": [
      "Home",
      "Sessions",
      "Assembly and Co-assembly of Metagenomic Raw Reads"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metagenomics bioinformatics at MGnify (2025)",
    "section": "",
    "text": "This is the course material for the Metagenomics bioinformatics at MGnify course (2025).\n\nOverview\nGain knowledge of the tools, processes and analysis approaches used in the field of metagenomics.\nThis course will cover the metagenomics data analysis workflow from the point of newly generated sequence data. Participants will explore the use of publicly available resources and tools to manage, share, analyse and interpret metagenomics data. The content will include issues of data quality control and how to submit to public repositories. While sessions will detail marker-gene and whole-genome shotgun (WGS) approaches; the primary focus will be on assembly-based approaches. Discussions will also explore considerations when assembling genome data, the analysis that can be carried out by MGnify on such datasets, and what downstream analysis options and tools are available"
  },
  {
    "objectID": "sessions/genome_uploader.html",
    "href": "sessions/genome_uploader.html",
    "title": "The genome_uploader: tutorial",
    "section": "",
    "text": "This session will simulate genomes registration and submission with the genome_uploader, a tool developed within MGnify to facilitate the upload of bins and MAGs to the ENA (European Nucleotide Archive). If you are interested, you can find the genome_uploader code on github, on pypi and bioconda.\nWhen you submit genomes to the ENA, you need to register a sample for every genome containing all the relevant metadata describing the genome and the sample of origin. The genome_uploader acts as the main linker to preserve sample metadata as much as possible. For every genome to register, you need an INSDC run or assembly accession associated to the genome in order for the script to inherit its relevant metadata. On top of those metadata, the script adds metadata specified by the user that are specific to the genome, like taxonomy, statistics, or the tools used to generate it. The metadata that ENA requires are descibed in the checklist for MAGs and for bins, respectively.\nFirst, let’s access the exercise folder. Open a terminal and type:\nYou will find a starting dataset of two genomes. Together with them, input_example.tsv is a table containing metadata about those genomes. It will like similar to this:\nWith columns indicating:\nAccording to ENA checklist’s guidelines, broad_environment describes the broad ecological context of a sample - desert, taiga, coral reef, … local_environment is more local - lake, harbour, cliff, … environmental_medium is either the material displaced by the sample, or the one in which the sample was embedded prior to the sampling event - air, soil, water, … For host-associated metagenomic samples, the three variables can be defined similarly to the following example for the chicken gut metagenome: “chicken digestive system”, “digestive tube”, “caecum”. More information can be found at ERC000050 for bins and ERC000047 for MAGs under field names “broad-scale environmental context”, “local environmental context”, “environmental medium”\nIf your genome was generated from raw reads available on the INSDC (including ENA and GenBank), the genome_uploader will automatically inherit relevant metadata for that sample to make. For example, if you are submitting a MAG generated from read SRR11910206, some of the sample metadata will be inherited for the genome sample registration (e.g. collection_date, isolation_source).\nTake a look at the GCS MIMAG checklist as a reference. You will notice that bins and MAGs checklists are very similar, as mandatory fields are the same. You can compare it with the bins checklist yourself.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#registering-and-submitting-genomes",
    "href": "sessions/genome_uploader.html#registering-and-submitting-genomes",
    "title": "The genome_uploader: tutorial",
    "section": "Registering and submitting genomes",
    "text": "Registering and submitting genomes\nAs explained before, you need to perform 4 steps to submit bins and MAGs:\n\nRegister a study\nRegister binned and/or MAG samples\nGenerate manifest files\nSubmit assemblies with Webin-CLI\n\nThe genome_uploader takes care of the last 3 steps. 2 and 3 are executed together, while the third one needs an extra command to submit previously generated files to ENA servers.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#accessing-yor-webin-profile",
    "href": "sessions/genome_uploader.html#accessing-yor-webin-profile",
    "title": "The genome_uploader: tutorial",
    "section": "Accessing yor Webin profile",
    "text": "Accessing yor Webin profile\nGo to this spreadsheet and reserve a Webin profile",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#registering-a-study",
    "href": "sessions/genome_uploader.html#registering-a-study",
    "title": "The genome_uploader: tutorial",
    "section": "Registering a study",
    "text": "Registering a study",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#registering-mag-samples-and-generating-manifest-files",
    "href": "sessions/genome_uploader.html#registering-mag-samples-and-generating-manifest-files",
    "title": "The genome_uploader: tutorial",
    "section": "Registering MAG samples and generating manifest files",
    "text": "Registering MAG samples and generating manifest files\nIt’s time to launch the genome_uploader.\n\n\n\n\n\n\nWarning\n\n\n\nReplace ENA_WEBIN and ENA_WEBIN_PASSWORD with your own credentials, and the METADATA_FILE and CENTRE_NAME parameters below with the right file ones!!\n\n\nexport ENA_WEBIN=\"Webin-XXX\"\nexport ENA_WEBIN_PASSWORD=\"Insert your password here\"\n\n# from the example_data folder:\n../genome_uploader/genomeuploader/genome_upload.py -u UPLOAD_STUDY --genome_info METADATA_FILE --mags --centre_name CENTRE_NAME\nWhere:\n\n-u UPLOAD_STUDY: study accession for genomes upload to ENA (in format ERPxxxxxx or PRJEBxxxxxx)\n---genome_info METADATA_FILE : genomes metadata file in tsv format\n--mags: replace --mags with --bins to upload bins: so EITHER OF THESE for either bin or MAG upload\n--centre_name CENTRE_NAME: name of the centre generating and uploading genomes\n\nThe logging output from this command will tell you which metadata objects have been accessed, and also where the output files have gone. This should be the example_data directory, but it will also have created a MAG_upload directory.\nBrowse the files here to see the manifests that have been created, which combine the ENA metadata and the provided metadata (e.g. binning software).\nYou can find the manifests in the MAG_upload/manifest_tests folder where you launched the script (should be in example_data).",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#submitting-your-genomes",
    "href": "sessions/genome_uploader.html#submitting-your-genomes",
    "title": "The genome_uploader: tutorial",
    "section": "Submitting your genomes",
    "text": "Submitting your genomes\nAfter checking that all needed manifests exist, it is necessary to use ENA’s webin-cli resource to upload genomes.\nFirst, download the resource in your folder.\ndownload_webin_cli -v 8.2.0\nThen, time to submit!\n\n\n\n\n\n\nWarning\n\n\n\nReplace MANIFEST_FILE below with the path to one of the manifest files that you just generated.\n\n\njava -jar webin-cli.jar -context=genome -manifest=MANIFEST_FILE -userName=${ENA_WEBIN} -password=${ENA_WEBIN_PASSWORD} -test -submit",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#try-it-yourself",
    "href": "sessions/genome_uploader.html#try-it-yourself",
    "title": "The genome_uploader: tutorial",
    "section": "Try it yourself",
    "text": "Try it yourself\n\n\n\n\n\n\nStep\n\n\n\nYour task is to generate another tsv table with sample metadata describing the genomes you want to register and submit.\n\n\nYou will be free to insert values as you wish, as long as regular expressions and mandatory fields described in the checklist are respected. You will need to select whether you are uploading bins or MAGs, and select your checklist accordingly.\n\n\n\n\n\n\nNote\n\n\n\nTo register a sample, a relative set of metadata must be filled according to the selected checklist. Some of them are mandatory, while some others are only recommended. The genome_uploader will automatically pick the right checklist depending on the input flag:\n\nGSC MIMAG for MAG samples (-mags)\nENA binned metagenome for binned samples (-bins)\n\n\n\nThe main difference between bins and MAGs lies in the uniqueness and the quality of your data. Within an ENA study, there should only be one MAG per species, which should be the highest quality representative genome per predicted species.\nHere we suggest a hypothetical scenario you might want to follow to make the metadata search more interesting\nSuppose your original dataset was small, very small, and it only generated three bins. Two of these bins represent the same species, but their statistics are extremely different. One assembled quite well, while the other one was highly contaminated. One would be considered “medium quality”, while the other “high quality”.\n\n\n\n\n\n\nTip\n\n\n\nThe INSDC defines a genome as high-quality when:\n\n\n\nAccording to what we previously mentioned, two of these bins could be categorised as MAGs, while the lower-quality bin would stay as a bin.\n\n\n\n\n\n\nTip\n\n\n\nTaxonomy lineages can be listed in either string (taxonomic names) or integer (taxids) format. An example of valid taxonomies you could use in this scenario could be:\n\nnames: d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Photobacterium;s__Photobacterium piscicola\nids: 1;131567;2759;33154;4751;451864;5204;452284;1538075;162474;742845;55193;76775",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#interested-in-retrieving-and-submitting-more-data-from-and-to-ena",
    "href": "sessions/genome_uploader.html#interested-in-retrieving-and-submitting-more-data-from-and-to-ena",
    "title": "The genome_uploader: tutorial",
    "section": "Interested in retrieving and submitting more data from and to ENA?",
    "text": "Interested in retrieving and submitting more data from and to ENA?\nHere is a link to a quick tour to use ENA for submission or retrieval of data.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html",
    "href": "sessions/mag_catalogues.html",
    "title": "MAG Catalogues",
    "section": "",
    "text": "MAGs 1 are an approach to deriving genome-resolved information from metagenomic datasets.\nMGnify’s MAG Catalogues are biome-specific, clustered, annotated collections of MAGs. Biomes are selected on the grounds of data availability, community interest, and project objectives.\n\n\n\n\n\n\n\n\nStepSearch the MGnify website\n\n\n\nSearch the All genomes list for the genus Jonquetella\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is that genus found?\nWhat do thise biomes have in common, and how does this align with the species found? 2\n\n\nNow, we want to get a FASTA sequence for this genus.\n\n\n\n\n\n\nQuestionFind the “best” MAG\n\n\n\nUsing what we’ve learned about QC on the course, look at the detail statistics of the Jonquetella MAGs. Which one is best? 3\n\n\n\n\n\n\n\n\nStepDownload the DNA sequence FASTA file of the “best” MAG\n\n\n\nWe will use it later.\n\n\n\n\n\nSourmash is a tool to compare DNA sequences against each other. The MGnify Genomes resource uses the sourmash library to create sketches (hashes) of every genome in every catalogues. You can query this index using your own sequences (typically MAGs you have retrieved from elsewhere or assembled yourself).\n\n\n\n\n\n\nStepQuery the catalogues using the Jonquetella MAG\n\n\n\nUse the MAG sequence FASTA file you earlier retrieved. 4\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is a match found for that query genome?\nWhat use cases can you think of for this kind of cross-catalogue search? 5\n\n\n\n\n\nThe MGnify website is just a client of the MGnify API 6.\nFor this part of the practical, there is a Jupyter Notebook you can follow along and try to complete the code blocks.\nTo open it on your training VM:\n\n\n\n\n\n\nStep\n\n\n\ncd ~/mgnify-notebooks\ngit status\n# make sure you're on the \"comparative_practice_2023\" branch\ntask edit-notebooks\nAfter a few seconds, some URLs will be printed in the terminal. Open the last one (http://127.0.0.1:8888/lab?token=.....), by right-clicking on the URL and selecting “Open Link”, or by copying-and-pasting it into a web browser like Chromium/Firefox.\n\n\n\n\n\n\n\n\nStepFind and open the ‘Search MGnify Genomes (course practical 2023)’ notebook in the ‘Python examples’ directory.\n\n\n\nFollow along the steps (completing some code blocks) in the notebook.\n\n\n\n\nThis notebook is based on a publicly accessible version. You can use this at any time.\n\nIt is available to use from your web browser, no installation needed: notebooks.mgnify.org\nYou can see a completed version of it, with all the outputs, on docs.mgnify.org\nYou can use a prebuilt docker image and our public notebooks repository: github.com/ebi-metagenomics/notebooks. This should work on any computer you can install Docker on.\nYou can try and install all the dependencies yourself ¯\\_(ツ)_/¯",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-1-finding-mags-by-taxonomy-on-the-mgnify-website",
    "href": "sessions/mag_catalogues.html#practical-1-finding-mags-by-taxonomy-on-the-mgnify-website",
    "title": "MAG Catalogues",
    "section": "",
    "text": "StepSearch the MGnify website\n\n\n\nSearch the All genomes list for the genus Jonquetella\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is that genus found?\nWhat do thise biomes have in common, and how does this align with the species found? 2\n\n\nNow, we want to get a FASTA sequence for this genus.\n\n\n\n\n\n\nQuestionFind the “best” MAG\n\n\n\nUsing what we’ve learned about QC on the course, look at the detail statistics of the Jonquetella MAGs. Which one is best? 3\n\n\n\n\n\n\n\n\nStepDownload the DNA sequence FASTA file of the “best” MAG\n\n\n\nWe will use it later.",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-2-query-mgnify-catalogues-using-sourmash",
    "href": "sessions/mag_catalogues.html#practical-2-query-mgnify-catalogues-using-sourmash",
    "title": "MAG Catalogues",
    "section": "",
    "text": "Sourmash is a tool to compare DNA sequences against each other. The MGnify Genomes resource uses the sourmash library to create sketches (hashes) of every genome in every catalogues. You can query this index using your own sequences (typically MAGs you have retrieved from elsewhere or assembled yourself).\n\n\n\n\n\n\nStepQuery the catalogues using the Jonquetella MAG\n\n\n\nUse the MAG sequence FASTA file you earlier retrieved. 4\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is a match found for that query genome?\nWhat use cases can you think of for this kind of cross-catalogue search? 5",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-3-query-mgnify-catalogues-using-sourmash-programmatically",
    "href": "sessions/mag_catalogues.html#practical-3-query-mgnify-catalogues-using-sourmash-programmatically",
    "title": "MAG Catalogues",
    "section": "",
    "text": "The MGnify website is just a client of the MGnify API 6.\nFor this part of the practical, there is a Jupyter Notebook you can follow along and try to complete the code blocks.\nTo open it on your training VM:\n\n\n\n\n\n\nStep\n\n\n\ncd ~/mgnify-notebooks\ngit status\n# make sure you're on the \"comparative_practice_2023\" branch\ntask edit-notebooks\nAfter a few seconds, some URLs will be printed in the terminal. Open the last one (http://127.0.0.1:8888/lab?token=.....), by right-clicking on the URL and selecting “Open Link”, or by copying-and-pasting it into a web browser like Chromium/Firefox.\n\n\n\n\n\n\n\n\nStepFind and open the ‘Search MGnify Genomes (course practical 2023)’ notebook in the ‘Python examples’ directory.\n\n\n\nFollow along the steps (completing some code blocks) in the notebook.\n\n\n\n\nThis notebook is based on a publicly accessible version. You can use this at any time.\n\nIt is available to use from your web browser, no installation needed: notebooks.mgnify.org\nYou can see a completed version of it, with all the outputs, on docs.mgnify.org\nYou can use a prebuilt docker image and our public notebooks repository: github.com/ebi-metagenomics/notebooks. This should work on any computer you can install Docker on.\nYou can try and install all the dependencies yourself ¯\\_(ツ)_/¯",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#footnotes",
    "href": "sessions/mag_catalogues.html#footnotes",
    "title": "MAG Catalogues",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMetagenome Assembled Genomes↩︎\nHint… what does anthropi in the species J. anthropi derive from?↩︎\nHint… each MAG’s detail page overview tab shows stats including completeness, contamination, and N50.↩︎\nIf you got lost earlier, download it from MGYG000304175.fna↩︎\nThere are interesting use cases for researchers (checking which environments a species is found in, checking whether a newly assembled genome is novel etc), as well as use cases for services like MGnify (cross-linking genomes between catalogues where those datasets are not clustered together).↩︎\nApplication Programming Interface↩︎",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/pangenome.html",
    "href": "sessions/pangenome.html",
    "title": "Pangenome analysis of metagenomic data",
    "section": "",
    "text": "We will first run through how to use our tool PopPUNK to build models to find subclusters within isolates data, then how to use these models (or pre-existing models) to assign MAG data to these clusters.\nWe anticipate the following timings:\n\nBuilding a PopPUNK model – 45 minutes.\nAssigning using a PopPUNK model – 30 minutes.\n\nPlease do not spend significantly longer than this on each one if you wish to complete the practical. You can move onto the next section at any time.\nWe will then show how to correct MAG data for incompleteness to give each gene a classification of core or accessory.\n\nFinding core and accessory genes from MAG data – 45 minutes.\n\nUse the sidebar to see the instructions for each part.\nThe files for all practicals are available on the virtual machine here:\nls ~/course_dir/data_dir/MGnify_training_course\nThe course prerequisities are already install on the VM. To activate the environment, run:\nsource ~/pangenome_mgnify_env/bin/activate\nIf the enviroment has been found correctly, you should see something similar to the following on your terminal\n(pangenome_mgnify_env) training@user:~$",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#preamble",
    "href": "sessions/pangenome.html#preamble",
    "title": "Pangenome analysis of metagenomic data",
    "section": "",
    "text": "We will first run through how to use our tool PopPUNK to build models to find subclusters within isolates data, then how to use these models (or pre-existing models) to assign MAG data to these clusters.\nWe anticipate the following timings:\n\nBuilding a PopPUNK model – 45 minutes.\nAssigning using a PopPUNK model – 30 minutes.\n\nPlease do not spend significantly longer than this on each one if you wish to complete the practical. You can move onto the next section at any time.\nWe will then show how to correct MAG data for incompleteness to give each gene a classification of core or accessory.\n\nFinding core and accessory genes from MAG data – 45 minutes.\n\nUse the sidebar to see the instructions for each part.\nThe files for all practicals are available on the virtual machine here:\nls ~/course_dir/data_dir/MGnify_training_course\nThe course prerequisities are already install on the VM. To activate the environment, run:\nsource ~/pangenome_mgnify_env/bin/activate\nIf the enviroment has been found correctly, you should see something similar to the following on your terminal\n(pangenome_mgnify_env) training@user:~$",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#building-poppunk-models",
    "href": "sessions/pangenome.html#building-poppunk-models",
    "title": "Pangenome analysis of metagenomic data",
    "section": "1. Building PopPUNK models",
    "text": "1. Building PopPUNK models\nWe will be using 112 B. uniformis isolate genomes (i.e. not MAG data). We are going to use these to define subspecies within the population using PopPUNK. These can be listed using:\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model\nInstallation instructions and an overview are available.\nPopPUNK is already installed on the VM. Confirm this by running poppunk --version.\n\nCreating the database\nThe first step to running PopPUNK on a new species is to create a ‘database’ which contains sketches of the genomes and calculates all of the core and accessory distances between the samples. We will be following the guidance in the relevant section of the documentation.\nFirst, navigate to the working directory and create a new directory:\ncd ~/course_dir/work_dir/Day_5 && mkdir pangenome && cd pangenome\nWe need to create a list of the input files. This needs to have the sample names and the location of files with their genomes. This can be created in many ways, here we will use a simple bash command:\npaste &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model | cut -d \".\" -f 1 ) &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model/MGYG*) &gt; rfile.txt\nWe can now create the input database with --create-db.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nUsing the documentation, create sketches of the fasta files, using a minimum k of 17, and maximum k of 33. Also, plot the fits to determine whether the values of k are suitable. The command will look like:\npoppunk --create-db --output b_uniformis_db --r-files rfile.txt --min-k &lt;replace&gt; --max-k &lt;replace&gt; --plot-fit 10 --threads 4\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo generate sketches and k vs. matching k plots:\npoppunk --create-db --output b_uniformis_db --r-files rfile.txt --min-k 17 --max-k 33 --plot-fit 10 --threads 4\nThis will run on 4 CPU cores to increase the speed. The other option is the range of k-mer lengths used, see the documentation for more information.\n\n\n\nHave a look at the plots in the output directory e.g. b_uniformis_db/b_uniformis_db_distanceDistribution.png and some of the fit examples such as b_uniformis_db_fit_example_1.pdf.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nCan you identify clusters by eye? Is the relationship between k and the proportion of matching k-mers linear?\n\n\nUsually we want to run some quality control on the data.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nTry this with a maximum core (pi) distance of 0.02 and a maximum accessory (a) distance of 1. Try also with different values and compare the results. The command will be:\npoppunk --qc-db --ref-db b_uniformis_db --max-pi-dist &lt;replace&gt; --max-a-dist &lt;replace &gt; --output b_uniformis_db_qc\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo run qc:\npoppunk --qc-db --ref-db b_uniformis_db --max-pi-dist 0.02 --max-a-dist 1 --output b_uniformis_db_qc\nThis removes outlying distances and poor quality samples.\nHowever if we run with a smaller core (pi) distance, this will remove half of the samples.\npoppunk --qc-db --ref-db b_uniformis_db --max-pi-dist 0.01 --max-a-dist 1 --output b_uniformis_db_qc_strict\nThis isn’t a good idea here as this core distance is too strict for a species as diverse as B. uniformis.\n\n\n\nThis is an example of how to condict quality control. In this case the data is all good quality and should all be retained.\n\n\nFitting a model\nWe now need to create a ‘model’ which determines a cutoff below which genomes are considered the same subspecies.\nThere are many options available, as detailed in the documentation.\nAs we have ‘small’ sample collection with strain-structure where distance distribution components are clearly separated, we’ll try the Bayesian Gaussian Mixture Model with two components:\npoppunk --fit-model bgmm --ref-db b_uniformis_db --output b_uniformis_BGMM_K2 --K 2\nFrom the output to the console we can see that everything is in one cluster (In Network Summary there is one component) and so we haven’t found any subspecies. Have a look at the output plot b_uniformis_BGMM_K2/b_uniformis_BGMM_K2_DPGMM_fit.png too.\nIt looks like adding an extra component (‘blob’) might help, so let’s try that and make three rather than two.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nFit the BGMM model using three components. How does it compare visually to using two components?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo run with more components:\npoppunk --fit-model bgmm --ref-db b_uniformis_db --output b_uniformis_BGMM_K3 --K 3\nThat looks better and there are some clusters (In Network Summary there are is seven/eight components), but if you look at the same plot again (b_uniformis_BGMM_K3/b_uniformis_BGMM_K3_DPGMM_fit.png) it doesn’t look like a great fit to the data.\n\n\n\nIn this case the data is actually a bit sparse to automatically get a good fit and we can do a much better job if we enrich the dataset with a few thousand MAGs and then use the ‘refine’ model mode. But for now we’ll take a shortcut and impose a cutoff that looks like it will work by using the threshold model.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nBased on the first cluster closest to the origin, come up with a suitable threshold.\nThe aim is to define a cutoff which separates the cluster closest to the origin from all others. The command will be:\npoppunk --fit-model threshold --ref-db b_uniformis_db --output b_uniformis_threshold --threshold &lt;replace&gt;\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nBased on where the first cluster closest to the origin is, it seems a threshold of core distance at 0.0025 is the best:\npoppunk --fit-model threshold --ref-db b_uniformis_db --output b_uniformis_threshold --threshold 0.0025\nThat looks better and there are some clusters (In Network Summary there are is seven components), but if you look at the same plot again (b_uniformis_BGMM_K3/b_uniformis_BGMM_K3_DPGMM_fit.png) it doesn’t look like a great fit to the data.\n\n\n\nThere actually isn’t one ‘perfect’ cutoff, which is supported by population genetic simulations (see this paper). So we are acutally free to choose a cutoff that defines subspecies which ‘work’ for us, and the models in PopPUNK are mostly a more convenient way of automating this to find useful clusters in general.\nAs you can see, this is all a bit fiddly and requires iteration. It is usually better to use a previously fitted and tested model, which we will cover in the next part.\n\n\nVisualising the results\nBut before we move on, let’s get a better look at the results. We can make a core genome tree and accessory genome embedding and plot the clusters interactively in the browser. First, run the poppunk_visualise command.\npoppunk_visualise --ref-db b_uniformis_db --model-dir b_uniformis_threshold --microreact --maxIter 100000 --output b_uniformis_db_viz\nHere, maxIter is being used to reduce the number embedding iterations as the dataset is small, just so the command runs quickly.\nNow, open up https://microreact.org/ in your browser and choose ‘Upload’. Drag and drop the .microreact file in the b_uniformis_db_viz directory to see the clusters and the tree. To change the tree layout, select the circular tree option in the top right corner.\nThe left-hand panel shows clustering by accessory genome distance, whilst the right-hand panel shows the tree generated from core genome distances. The points are coloured by which sequence cluster they were assigned to by PopPUNK.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nCheck whether the cluster assignments (colours of tips/points) matches up with the phylogeny. Are the cluster assignments in agreement with the core genome tree and the accessory distances?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nAs we only used a core genome threshold, it is likely that the cluster assignments will be in better aggreement with the core distances rather than the accessory distances. Using different models to identify sequence clusters (e.g. BGMM) will give different results.\n\n\n\n\n\nFurther analysis\nIf you have time, try playing around with different methods of fitting PopPUNK models in the documentation.\nYou can experiment with HDBSCAN, a method for automatically detecting the number and position of clusters.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nHow do the number of identified sequence clusters compare between using BGMM, HDBSCAN and threshold models?\n\n\nOnce you have a model fitted using BGMM or HDBSCAN, you can also refine it. This method takes a previously identified within-strain boundary and moves it to optimise the network score of the strain cluster network. We always recommend refining a previous fit, as it may significantly improve strain assignments.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nAgain, how do the number of identified sequence clusters compare when using refinement?\n\n\nYou can also play around with visualisation tools such as Cytoscape. Use the poppunk_visualise tool to generate a cytoscape output following the documentation. This will allow you to visualise the strain cluster network, the components of which are the strains detected by PopPUNK. Note: this will generate a file for each component, as well as the whole network, enabling visualisation of the whole network or just parts of it. To open the file, open Cytoscape -&gt; File -&gt; Import -&gt; Network from File and them open a .gml file.",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#assigning-using-poppunk-models",
    "href": "sessions/pangenome.html#assigning-using-poppunk-models",
    "title": "Pangenome analysis of metagenomic data",
    "section": "2. Assigning using PopPUNK models",
    "text": "2. Assigning using PopPUNK models\nIt is faster to ‘assign’ new samples to an existing definition of subspecies. This has the bonus that their names will be consistent across studies.\nTypically, you can download an existing database with a fit from https://www.bacpop.org/poppunk-databases. If you have a fit for a new species please send it to us and we can share it here.\nThere is no fit for B. uniformis (yet…) so we’ll use the one we just made.\n\nUsing poppunk_assign to assign MAGs to subspecies\nNow we’ll work with a large collection of MAGs. These are the B. uniformis MAGs from MGnify with &gt;95% completeness and &lt;5% contamination. They can be listed here\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign\nPopPUNK distances are relatively robust to missing sequence content seen in MAGs, but less able to deal with contamination.\nAgain, navigate to the work directory and create the list of input files for poppunk:\ncd ~/course_dir/work_dir/Day_5/pangenome\npaste &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign | cut -d \".\" -f 1 ) &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign/*.fasta) &gt; qfile.txt\nOne small problem is that these MAGs also contain the isolates from before. PopPUNK will refuse to assign these without unique names. Here’s a bash incantation to remove the duplicates:\ncut -f 1 rfile.txt &gt; ref_names.txt && grep -v -w -f ref_names.txt qfile.txt &gt; qfile_nodups.txt\nmv qfile_nodups.txt qfile.txt\nThe command is relatively simple, we need to provide the database, the model directory and the input ‘query’ genomes to assign. Quality control is ‘built-in’:\npoppunk_assign --db b_uniformis_db --model-dir b_uniformis_threshold --query qfile.txt --output b_uniformis_queries --threads 4 --max-merge 3 --run-qc --max-a-dist 0.8\nThe b_uniformis_queries_clusters.csv file contains the subspecies assignments.\nThe visualisation command is a bit more involved as we need to point to both directories and the model directory:\npoppunk_visualise --ref-db b_uniformis_db --query-db b_uniformis_queries --model-dir b_uniformis_threshold --microreact --output b_uniformis_query_viz --threads 4 --maxIter 10000000 --previous-query-clustering b_uniformis_queries/b_uniformis_queries_clusters.csv --previous-clustering b_uniformis_threshold/b_uniformis_threshold_clusters.csv\nLoad the .microreact output file in the b_uniformis_query_viz directory in Microreact again to see the output.\nUse the menu under the ‘eye’ to change from reference/query colour (‘Status’) to ‘Cluster_Cluster’ to see the clusters.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nWhere do the new genomes sit on the tree relative to the old ones? Do they form a new clade, or sit across existing clades?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nYou’ll see that some MAGs formed new clusters (‘novel clusters’), whilst others were merged, as the new genomes connected to multiple clusters generated in the original database.\n\n\n\n\n\nUpdating the database\nIt is possible to permanently add the query genomes to the database, so future users can make use of novel cluster assignments. Simply add --update-db to the assignment command above. This is beyond the scope of this practical but is documented here and here.\n\n\nFurther analysis\nIf you have time, try generating another visualisation of the query network using Cytoscape.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nHow does this network compare to the original one from the previous section, when you only built using isolate genomes and not MAG data?\n\n\nAlso, try building a PopPUNK model as before, but only using the MAG data you used from assignment.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nHow do the distance distributions compare between the isolate data and MAG data? Does a threshold model work with the data, or would an automated method for model fitting work better?",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#finding-core-and-accessory-genes",
    "href": "sessions/pangenome.html#finding-core-and-accessory-genes",
    "title": "Pangenome analysis of metagenomic data",
    "section": "3. Finding core and accessory genes",
    "text": "3. Finding core and accessory genes\nWe can use a probabilistic model to correct for the fact that we know MAGs are missing some genes. If we don’t do this then we will systematically under-estimate their population frequency, and end up with nothing in the core genome.\n\nCELEBRIMBOR: a pipeline for pangenome analysis and threshold correction\nThe CELEBRIMBOR prerequisities are already install on the VM. The source code files can be found in the CELEBRIMBOR directory which can be listed here:\nls ~/CELEBRIMBOR-1.1.2\nAs well be editing the config files, make a copy in your working directory:\ncp -r ~/CELEBRIMBOR-1.1.2 ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR\nThe files we will be analysing can be listed here:\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR\nThis archive contains two directories. CELEBRIMBOR_MAGs contains the fasta files we will be analysing. results contains the Bakta and CheckM outputs which were generated previously from these genomes. As these are the slowest parts of the analysis, we have provided them to allow you to generate results faster.\nAs we’ll be updating results, copy the whole directory to your working directory:\ncp -r ~/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR/results ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\nCELEBRIMBOR is a snakemake pipeline which enables automated gene annotation, clustering, completeness estimation and core threshold adjustment. Snakemake allows workflows to be re-run if workflows stop prematurely.\nTo prevent CELEBRIMBOR from re-running Bakta and CheckM, I have edited the source code. If you do your own analysis outside of this practical, you will run the full workflow using identical commands.\nDocumentation for CELEBRIMBOR can be found here.\nSnakemake reads a config.yaml file to assign parameters. Navigate to the CELEBRIMBOR directory, and update the config.yaml file with the appropriate parameters.\ncd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR\nvim config.yaml\nTo start typing in vim, type i. To close vim, first use the esc key, and then WITH saving type, :wq, or WITHOUT saving, type :q!\n#output directory\noutput_dir: /home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\n\n# dir with genome fasta files, must have '.fasta' extension, to convert use 'rename .fa .fasta  *.fa' (e.g. if extensions are .fa)\ngenome_fasta: /home/training/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR/CELEBRIMBOR_MAGs\n\n# path to bakta DB:\nbakta_db: /home/training/course_dir/data_dir/db-light\n\n# cgt executable parameters\ncgt_exe: /home/training/.cargo/bin/cgt_bacpop\ncgt_breaks: 0.05,0.95\ncgt_error: 0.05\n\n# choice of clustering method, either \"mmseqs2\" or \"panaroo\"\nclustering_method: \"mmseqs2\"\n\n# must be one of \"strict\", \"moderate\" or \"sensitive\"\npanaroo_stringency: \"strict\"\nNote: paths in the config.yaml file cannot contain ~ symbols.\nTo run CELEBRIMBOR, simply run the below command. Snakemake will read the config.yaml file and run CELEBRIMBOR on the associated files, avoiding Bakta and CheckM as these results have already been generated.\nsnakemake --cores 4\nThis will run for a few minutes. You’ll see a number of tools being run, including MMseqs2 for clustering, and cgt for frequency threshold adjustment.\nWhile you’re waiting, feel free to take a look at the CELEBRIMBOR or PopPUNK papers.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nOnce CELEBRIMBOR has finished running, take a look at the outputs printed to the console. What are the new adjusted core and rare thresholds?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nYou’ll observe the new frequency thresholds printed to the console. Here, the core threshold was reduced from 95% to 92.73%, whilst the rare threshold was increased from 5% to 10%.\nCore threshold: &gt;= 102 observations or &gt;= 92.73% frequency\nRare threshold: &lt;= 11 observations or &lt;= 10.00% frequency\n\n\n\nTake a look at the output files by running:\ncd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results && ls\nThe pangenome_summary.tsv describes the assignments of each gene family to a frequency compartment based on the raw frequency values (column order: gene name, gene annotation, frequency, frequency compartment).\ncgt_output.txt details the adjusted frequency compartment assignments calculated by CELEBRIMBOR (column order: gene name, gene count, adjusted frequency compartment).\nUsing R, you can plot the different between the adjusted and unadjusted results by updating the script plot_frequency.R with the paths to pangenome_summary.tsv and cgt_output.txt.\nCopy the R script to the working directory and edit:\ncp ~/course_dir/data_dir/MGnify_training_course/plot_frequency.R ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\nvim plot_frequency.R\nEdit the file below, using esc key and dd to delete whole lines if necessary\nlibrary(ggplot2)\n\n# read in data\npangenome.summary &lt;- read.csv(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/pangenome_summary.tsv\", sep = \"\\t\", header=FALSE)\ncolnames(pangenome.summary) &lt;- c(\"gene_name\", \"gene_family\", \"frequency\", \"compartment_freq\")\npangenome.summary$gene_family &lt;- NULL\n\ncgt.summary &lt;- read.csv(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/cgt_output.txt\", sep = \"\\t\", header=TRUE)\ncolnames(cgt.summary) &lt;- c(\"gene_name\", \"count\", \"compartment_adjusted\")\n\n# Merge data\ntotal.df &lt;- merge(pangenome.summary, cgt.summary, by = \"gene_name\")\n\n# stack data\nstacked.df &lt;- data.frame(Type = \"Unadjusted\", Compartment = total.df$compartment_freq)\nstacked.df &lt;- rbind(stacked.df, data.frame(Type = \"Adjusted\", Compartment = total.df$compartment_adjusted))\nstacked.df$Compartment &lt;- factor(stacked.df$Compartment, levels = c(\"rare\", \"middle\", \"core\"))\nstacked.df$Type &lt;- factor(stacked.df$Type, levels = c(\"Unadjusted\", \"Adjusted\"))\n\n# plot data\np &lt;- ggplot(stacked.df, aes(x=Type, fill = Type)) + facet_grid(Compartment~., scales = \"free_y\") + geom_bar() + xlab(\"Pangenome analysis type\") + ylab(\"Gene count\") + theme(legend.position = \"none\")\nggsave(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/adjusted_gene_frequency.png\", plot = p)\nRun the script from the R terminal:\nR\nsource(\"plot_frequency.R\")\nUse Ctrl + Z to close the R terminal.\nTake a look at adjusted_gene_frequency.png by opening the Files application and navigating to Home -&gt; course_dir -&gt; work_dir -&gt; Day_5 -&gt; pangenome -&gt; CELEBRIMBOR_results.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nIt should look something like this:\n\n\n\nEffect of frequency adjustment\n\n\nObserve that the core genome and rare genome sizes increase after adjustment, whilst the middle or intermediate decreases.\n\n\n\nThere are other files that can be used for downstream analysis, such as presence_absence_matrix.txt which defines in which genomes gene families are found, as well as the annotated directory, which contains gene annotations generated using Bakta.\n\n\nFurther analysis\nTry running CELEBRIMBOR with different parameters, such as cgt_breaks which defines the original rare and core thresholds to adjust, or cgt_error, which defines the false negative rate of CELEBRIMBOR. Replot these using the R script and compare to your previous results.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nWhat does altering these parameter do to the number of core, middle and rare genes identified?\n\n\nYou can also try running with clustering_method set to panaroo, which uses Panaroo, a more accurate clustering method but less scalable than MMseqs2 (this will likely take a while to run). Also, when running Panaroo, try setting panaroo_stringency to moderate or senstive. Details on the effect of these parameters can be found here.\n\n\n\n\n\n\nImportantChallenge\n\n\n\nPlot the output of Panaroo against that of MMseqs in R, and see how using different clustering algorithms impacts the number of core, middle and rare genes.",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#example-analysis",
    "href": "sessions/pangenome.html#example-analysis",
    "title": "Pangenome analysis of metagenomic data",
    "section": "Example analysis",
    "text": "Example analysis\nExample analysis for all of today’s workshop is available here:\nls ~/course_dir/data_dir/MGnify_training_course/example_analysis/pangenome",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  }
]